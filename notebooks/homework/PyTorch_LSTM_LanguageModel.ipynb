{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业一：语言模型 Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 本次作业通过PyTorch搭建LSTM语言模型\n",
    "* 数据集采用ROC故事语料库，为减少单词量已将大部分名字转换为Bob和Sue\n",
    "* 数据集包含以下内容：\n",
    "\n",
    "文件名|说明\n",
    ":-:|:-:\n",
    "bobsue.lm.train.txt | 语言模型训练数据\n",
    "bobsue.lm.dev.txt | 语言模型验证数据\n",
    "bobsue.lm.test.txt | 语言模型测试数据\n",
    "bobsue.voc.txt | 词汇表数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先导入这次作业需要的包，并设置随机种子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设定计算设备与数据集路径**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.4.0\n",
      "------------------------------------------------------------\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name:\n",
      "\t GeForce RTX 2080 Ti\n",
      "\t GeForce RTX 2080 Ti\n",
      "CUDA Current Device Index: 0\n",
      "------------------------------------------------------------\n",
      "Data Path: /media/bnu/data/nlp-practice/language-model\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "data_path = Path('/media/bnu/data/nlp-practice/language-model')\n",
    "\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "print('-' * 60)\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Device Count:', torch.cuda.device_count())\n",
    "    print('CUDA Device Name:')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print('\\t', torch.cuda.get_device_name(i))\n",
    "    print('CUDA Current Device Index:', torch.cuda.current_device())\n",
    "    print('-' * 60)\n",
    "print('Data Path:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义单词表类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义`Vocab`类用于存储单词表\n",
    "* `Vocab`类当中包含了单词(token)与索引(index)之间的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab_path):\n",
    "        self.stoi = {}  # token -> index (dict)\n",
    "        self.itos = []  # index -> token (list)\n",
    "        \n",
    "        with open(vocab_path) as f:\n",
    "            # bobsue.voc.txt中，每一行是一个单词\n",
    "            for w in f.readlines():\n",
    "                w = w.strip()\n",
    "                if w not in self.stoi:\n",
    "                    self.stoi[w] = len(self.itos)\n",
    "                    self.itos.append(w)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词表大小： 1498\n",
      "------------------------------------------------------------\n",
      "样例（单词 -> 索引）：\n",
      "[('<s>', 0), ('</s>', 1), ('.', 2), ('to', 3), ('Bob', 4)]\n",
      "------------------------------------------------------------\n",
      "样例（索引 -> 单词）：\n",
      "[(0, '<s>'), (1, '</s>'), (2, '.'), (3, 'to'), (4, 'Bob')]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(data_path / 'bobsue.voc.txt')\n",
    "print('单词表大小：', len(vocab))\n",
    "print('-' * 60)\n",
    "print('样例（单词 -> 索引）：')\n",
    "print(list(vocab.stoi.items())[:5])\n",
    "print('-' * 60)\n",
    "print('样例（索引 -> 单词）：')\n",
    "print(list(enumerate(vocab.itos))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义`Corpus`类读取训练集、验证集、测试集语料\n",
    "* 语料文件中每一行都是一个句子，也就是我们训练时的一份样本\n",
    "* 将语料中的句子读入后，根据`Vocab`转换成索引列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, data_path, sort_by_len=False):\n",
    "        self.vocab = Vocab(data_path / 'bobsue.voc.txt')\n",
    "        self.sort_by_len = sort_by_len\n",
    "        self.train_data = self.tokenize(data_path / 'bobsue.lm.train.txt')\n",
    "        self.valid_data = self.tokenize(data_path / 'bobsue.lm.dev.txt')\n",
    "        self.test_data = self.tokenize(data_path / 'bobsue.lm.test.txt')\n",
    "        \n",
    "    def tokenize(self, text_path):\n",
    "        with open(text_path) as f:\n",
    "            index_data = []  # 索引数据，存储每个样本的单词索引列表\n",
    "            for s in f.readlines():\n",
    "                index_data.append(\n",
    "                    self.sentence_to_index(s)\n",
    "                )\n",
    "        if self.sort_by_len:  # 为了提升训练速度，可以考虑将样本按照长度排序，这样可以减少padding\n",
    "            index_data = sorted(index_data, key=lambda x: len(x), reverse=True)\n",
    "        return index_data\n",
    "    \n",
    "    def sentence_to_index(self, s):\n",
    "        return [self.vocab.stoi[w] for w in s.split()]\n",
    "    \n",
    "    def index_to_sentence(self, x):\n",
    "        return ' '.join([self.vocab.itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 6036\n",
      "验证集句子数目： 750\n",
      "测试集句子数目： 750\n",
      "------------------------------------------------------------\n",
      "训练集总共单词数目： 71367\n",
      "验证集总共单词数目： 8707\n",
      "测试集总共单词数目： 8809\n",
      "------------------------------------------------------------\n",
      "训练集预测单词数目： 65331\n",
      "验证集预测单词数目： 7957\n",
      "测试集预测单词数目： 8059\n",
      "------------------------------------------------------------\n",
      "数据样本：\n",
      "[0, 16, 235, 372, 10, 60, 3, 75, 618, 39, 2, 1]\n",
      "<s> She ate quickly and asked to be taken home . </s>\n",
      "[0, 38, 192, 222, 32, 31, 4, 2, 1]\n",
      "<s> The girl broke up with Bob . </s>\n",
      "[0, 7, 842, 2, 1]\n",
      "<s> Sue apologized . </s>\n",
      "[0, 12, 150, 18, 8, 261, 3, 546, 102, 5, 1097, 2, 1]\n",
      "<s> He tried for a year to break into the market . </s>\n",
      "[0, 200, 706, 14, 5, 26, 15, 427, 228, 2, 1]\n",
      "<s> So far , the day had gone well . </s>\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_path, sort_by_len=False)\n",
    "print('训练集句子数目：', len(corpus.train_data))\n",
    "print('验证集句子数目：', len(corpus.valid_data))\n",
    "print('测试集句子数目：', len(corpus.test_data))\n",
    "print('-' * 60)\n",
    "print('训练集总共单词数目：', sum([len(x) for x in corpus.train_data]))\n",
    "print('验证集总共单词数目：', sum([len(x) for x in corpus.valid_data]))\n",
    "print('测试集总共单词数目：', sum([len(x) for x in corpus.test_data]))\n",
    "print('-' * 60)\n",
    "print('训练集预测单词数目：', sum([len(x) - 1 for x in corpus.train_data]))\n",
    "print('验证集预测单词数目：', sum([len(x) - 1 for x in corpus.valid_data]))\n",
    "print('测试集预测单词数目：', sum([len(x) - 1 for x in corpus.test_data])) \n",
    "print('-' * 60)\n",
    "print('数据样本：')\n",
    "for i in range(5):\n",
    "    print(corpus.train_data[i])\n",
    "    print(corpus.index_to_sentence(corpus.train_data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语言模型的DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里使用PyTorch中的`DataSet`来构建我们自己的语言模型数据集\n",
    "* 我们自定义的类继承`DataSet`后，要实现`__len__`与`__getitem__`方法\n",
    "* 根据语言模型定义，每个样本的输入是前n-1个单词，预测目标为后n-1个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BobSueLMDataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, index_data):\n",
    "        self.index_data = index_data\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        # 根据语言模型定义，这里我们要用前n-1个单词预测后n-1个单词\n",
    "        return self.index_data[i][:-1], self.index_data[i][1:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 6036\n",
      "训练集样本：\n",
      "\t输入： [0, 7, 208, 601, 28, 10, 1276, 25, 5, 709, 507, 2]\n",
      "\t      <s> Sue sat behind him and stared at the cute guy .\n",
      "\t目标： [7, 208, 601, 28, 10, 1276, 25, 5, 709, 507, 2, 1]\n",
      "\t      Sue sat behind him and stared at the cute guy . </s>\n"
     ]
    }
   ],
   "source": [
    "train_set = BobSueLMDataSet(corpus.train_data)\n",
    "print('训练集大小：', len(train_set))\n",
    "print('训练集样本：')\n",
    "print('\\t输入：', train_set[10][0])\n",
    "print('\\t     ', corpus.index_to_sentence(train_set[10][0]))\n",
    "print('\\t目标：', train_set[10][1])\n",
    "print('\\t     ', corpus.index_to_sentence(train_set[10][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语言模型的DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 因为数据集中每个样本的长度不同，我们需要自定义`collate_fn`来处理这个问题\n",
    "* 为了解决这个问题我搞了一下午，尝试了很多办法，现在这个应该是相对“优雅”的解决方案\n",
    "* 这部分我参考了[PyTorch论坛中的方法](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_collate_fn(batch):\n",
    "    # 这里输入的batch格式为[(input_1, target_1), ... ,(input_n, target_n)]\n",
    "    # 我们要将其格式转换为[(input_1, ... , input_n), (target_1, ... , target_n)]\n",
    "    batch = list(zip(*batch))\n",
    "    \n",
    "    # 生成长度列表\n",
    "    lengths = torch.LongTensor([len(x) for x in batch[0]]).to(device)\n",
    "    \n",
    "    # 对输入和目标进行padding\n",
    "    inputs = [torch.LongTensor(x).to(device) for x in batch[0]]\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
    "    targets = [torch.LongTensor(x).to(device) for x in batch[1]]\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    # 因为目标中不存在编号为0的单词，所以目标中为0的位置为padding，由此生成mask矩阵\n",
    "    mask = (targets != 0).float().to(device)\n",
    "    \n",
    "    # 在之后的训练中因为还要进行pack_padded_sequence操作，所以在这里按照长度降序排列\n",
    "    lengths, perm_index = lengths.sort(descending=True)\n",
    "    inputs = inputs[perm_index]\n",
    "    targets = targets[perm_index]\n",
    "    mask = mask[perm_index]\n",
    "    \n",
    "    return inputs, targets, lengths, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入：\n",
      "tensor([[   0,   12,   77,    5,   62,    3,    5, 1250,    3,  108,  111,    6,\n",
      "          500,   31,   20,    2],\n",
      "        [   0,  159,  713,  353,   17,   33,    3,  108,  111,    6,  553,   28,\n",
      "           55,  164,    2,    0],\n",
      "        [   0,    4,   35,  113,  260,  652,    3,   36,    3,    5,  363,  141,\n",
      "           67,    0,    0,    0],\n",
      "        [   0,   12,  231,   25,    5,  538,   31,   11,   95,   25,  660,    2,\n",
      "            0,    0,    0,    0],\n",
      "        [   0,   12,  599,   10,   13,   77,    5,  267,    3,   58,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   0,    7,  255,   32,   31,    8,  731, 1383,    2,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   0,   16,    6,  431,   24,    5, 1078,    2,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   0,    4,  255,   32,  146,  369,  285,    2,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]], device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "目标：\n",
      "tensor([[  12,   77,    5,   62,    3,    5, 1250,    3,  108,  111,    6,  500,\n",
      "           31,   20,    2,    1],\n",
      "        [ 159,  713,  353,   17,   33,    3,  108,  111,    6,  553,   28,   55,\n",
      "          164,    2,    1,    0],\n",
      "        [   4,   35,  113,  260,  652,    3,   36,    3,    5,  363,  141,   67,\n",
      "            1,    0,    0,    0],\n",
      "        [  12,  231,   25,    5,  538,   31,   11,   95,   25,  660,    2,    1,\n",
      "            0,    0,    0,    0],\n",
      "        [  12,  599,   10,   13,   77,    5,  267,    3,   58,    1,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   7,  255,   32,   31,    8,  731, 1383,    2,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [  16,    6,  431,   24,    5, 1078,    2,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   4,  255,   32,  146,  369,  285,    2,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]], device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "Mask：\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "每个样本的实际长度：\n",
      "tensor([16, 15, 13, 12, 10,  9,  8,  8], device='cuda:0')\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=lm_collate_fn\n",
    ")\n",
    "inputs, targets, lengths, mask = next(iter(test_loader))\n",
    "print('输入：')\n",
    "print(inputs)\n",
    "print('-' * 60)\n",
    "print('目标：')\n",
    "print(targets)\n",
    "print('-' * 60)\n",
    "print('Mask：')\n",
    "print(mask)\n",
    "print('-' * 60)\n",
    "print('每个样本的实际长度：')\n",
    "print(lengths)\n",
    "print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下面实现一个基于LSTM的网络架构\n",
    "* 输入数据经过一个Embedding层后输入给LSTM，对于LSTM每一个输出经过一个线性层作为输出\n",
    "* 模型`forward`过程中使用`pack_padded_sequence`和`pad_packed_sequence`方法处理变长输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMLM(\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (embed): Embedding(1498, 200)\n",
       "  (lstm): LSTM(200, 200, batch_first=True)\n",
       "  (linear): Linear(in_features=200, out_features=1498, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMLM(nn.Module):\n",
    "    \"\"\"语言模型网络架构\n",
    "    \n",
    "    Args:\n",
    "        n_words: 词表中的单词数目\n",
    "        n_embed: 词向量维度\n",
    "        n_hidden: LSTM隐含状态的维度\n",
    "        dropout: Dropout概率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_words, n_embed=200, n_hidden=200, dropout=0.5):\n",
    "        super(LSTMLM, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(n_words, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, n_words)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        # inputs shape: (batch_size, max_length)\n",
    "        # x_emb shape: (batch_size, max_length, embed_size)\n",
    "        x_emb = self.drop(self.embed(inputs))\n",
    "        \n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_emb, lengths, batch_first=True\n",
    "        )\n",
    "        # 这里LSTM的h_0,c_0使用全0的默认初始化，LSTM层经过后丢弃\n",
    "        packed_out, _ = self.lstm(packed_emb)\n",
    "        # x_out shape: (batch_size, max_length, hidden_size)\n",
    "        x_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # outputs shape: (batch, max_length, vocab_size)\n",
    "        return self.linear(self.drop(x_out))\n",
    "        \n",
    "model = LSTMLM(len(corpus.vocab), 200, 200)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输入Shape： torch.Size([8, 16])\n",
      "模型输出Shape： torch.Size([8, 16, 1498])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, lengths, mask = next(iter(test_loader))\n",
    "outputs = model(inputs, lengths)\n",
    "print('模型输入Shape：', inputs.shape)\n",
    "print('模型输出Shape：', outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我们batch中存在padding不能直接使用`CrossEntropyLoss`\n",
    "* 这里需要在原有loss基础上乘以mask矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskCrossEntropyLoss, self).__init__()\n",
    "        self.celoss = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, targets, mask):\n",
    "        # outputs shape: (batch_size * max_len, vocab_size)\n",
    "        outputs = outputs.view(-1, outputs.size(2))\n",
    "        # targets shape: (batch_size * max_len)\n",
    "        targets = targets.view(-1)\n",
    "        # mask shape: (batch_size * max_len)\n",
    "        mask = mask.view(-1)\n",
    "        loss = self.celoss(outputs, targets) * mask\n",
    "        return torch.sum(loss) / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失值： tensor(7.3313, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, lengths, mask = next(iter(test_loader))\n",
    "outputs = model(inputs, lengths)\n",
    "criterion = MaskCrossEntropyLoss().to(device)\n",
    "loss = criterion(outputs, targets, mask)\n",
    "print('损失值：', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语言模型学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 数据与模型都已经定义好了，接下来实现`LMLearner`类完成模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class LMLearner:\n",
    "    def __init__(self, corpus, n_embed=200, n_hidden=200, dropout=0.5, \n",
    "                 batch_size=128, early_stopping_round=5):\n",
    "        self.corpus = corpus\n",
    "        self.batch_size = batch_size\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        self.model = LSTMLM(len(corpus.vocab), n_embed, n_hidden, dropout).to(device)\n",
    "        self.criterion = MaskCrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "    def fit(self, num_epochs):\n",
    "        # 定义训练集dataloader\n",
    "        train_set = BobSueLMDataSet(self.corpus.train_data)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lm_collate_fn\n",
    "        )\n",
    "        \n",
    "        # 定义验证集dataloader\n",
    "        valid_set = BobSueLMDataSet(self.corpus.valid_data)\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            dataset=valid_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lm_collate_fn\n",
    "        )\n",
    "        \n",
    "        # 记录验证集没有提高的轮数，用于EarlyStopping\n",
    "        no_improve_round = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):            \n",
    "            train_loss, train_acc, train_words = self._make_train_step(train_loader)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}:')\n",
    "                print('Train Step --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "                    train_loss, train_acc, train_words))\n",
    "            # 记录训练信息\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "        \n",
    "            valid_loss, valid_acc, valid_words = self._make_valid_step(valid_loader)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print('Valid Step --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "                    valid_loss, valid_acc, valid_words))\n",
    "            self.history['valid_loss'].append(valid_loss)\n",
    "            self.history['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # 根据验证集的准确率进行EarlyStopping\n",
    "            if self.history['valid_acc'][-1] < max(self.history['valid_acc']):\n",
    "                no_improve_round += 1\n",
    "            else:\n",
    "                no_improve_round = 0\n",
    "            if no_improve_round == self.early_stopping_round:\n",
    "                print(f'Early Stopping at Epoch {epoch+1}')\n",
    "                break\n",
    "            \n",
    "        \n",
    "    def predict(self):\n",
    "        test_set = BobSueLMDataSet(self.corpus.test_data)\n",
    "        # 这里注意，为了方便之后分析不要shuffle，batch_size设置为1\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=test_set,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=lm_collate_fn\n",
    "        )\n",
    "        \n",
    "        # 验证模式\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 正确预测的数目，单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        # 预测结果字典，包含preds和targets\n",
    "        test_result = defaultdict(list) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, lengths, mask in test_loader:\n",
    "                # 计算模型输出\n",
    "                outputs = self.model(inputs, lengths)\n",
    "                \n",
    "                # 统计当前预测正确的数目\n",
    "                total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "                # 统计当前总预测单词数\n",
    "                total_words += torch.sum(lengths).item()\n",
    "                \n",
    "                # 记录结果\n",
    "                test_result['preds'].append(outputs.argmax(-1).data.cpu().numpy()[0])\n",
    "                test_result['targets'].append(targets.data.cpu().numpy()[0])\n",
    "                \n",
    "                # 计算模型Mask交叉熵损失\n",
    "                loss = self.criterion(outputs, targets, mask)\n",
    "                # 统计总损失\n",
    "                total_loss += loss.item() * torch.sum(mask).item()\n",
    "        return total_loss / total_words, total_correct / total_words, total_words, test_result\n",
    "        \n",
    "    def _make_train_step(self, train_loader):\n",
    "        # 训练模式\n",
    "        self.model.train()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 正确预测的数目，单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        for inputs, targets, lengths, mask in train_loader:\n",
    "            # 计算模型输出\n",
    "            outputs = self.model(inputs, lengths)\n",
    "            \n",
    "            # 统计当前预测正确的数目\n",
    "            total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "            # 统计当前总预测单词数\n",
    "            total_words += torch.sum(lengths).item()\n",
    "            \n",
    "            # 计算模型Mask交叉熵损失\n",
    "            loss = self.criterion(outputs, targets, mask)\n",
    "            # 统计总损失\n",
    "            total_loss += loss.item() * torch.sum(mask).item()\n",
    "                        \n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return total_loss / total_words, total_correct / total_words, total_words\n",
    "    \n",
    "    def _make_valid_step(self, valid_loader):\n",
    "        # 验证模式\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 正确预测的数目，单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, lengths, mask in valid_loader:\n",
    "                # 计算模型输出\n",
    "                outputs = self.model(inputs, lengths)\n",
    "                \n",
    "                # 统计当前预测正确的数目\n",
    "                total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "                # 统计当前总预测单词数\n",
    "                total_words += torch.sum(lengths).item()\n",
    "                \n",
    "                # 计算模型Mask交叉熵损失\n",
    "                loss = self.criterion(outputs, targets, mask)\n",
    "                # 统计总损失\n",
    "                total_loss += loss.item() * torch.sum(mask).item()\n",
    "        return total_loss / total_words, total_correct / total_words, total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里设定好参数就可以开始训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train Step --> Loss: 3.729, Acc: 0.288, Words: 65331\n",
      "Valid Step --> Loss: 3.627, Acc: 0.304, Words: 7957\n",
      "Epoch 20:\n",
      "Train Step --> Loss: 3.433, Acc: 0.309, Words: 65331\n",
      "Valid Step --> Loss: 3.442, Acc: 0.327, Words: 7957\n",
      "Epoch 30:\n",
      "Train Step --> Loss: 3.263, Acc: 0.324, Words: 65331\n",
      "Valid Step --> Loss: 3.378, Acc: 0.333, Words: 7957\n",
      "Epoch 40:\n",
      "Train Step --> Loss: 3.144, Acc: 0.331, Words: 65331\n",
      "Valid Step --> Loss: 3.352, Acc: 0.338, Words: 7957\n",
      "Epoch 50:\n",
      "Train Step --> Loss: 3.059, Acc: 0.338, Words: 65331\n",
      "Valid Step --> Loss: 3.336, Acc: 0.335, Words: 7957\n",
      "Early Stopping at Epoch 54\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "learner = LMLearner(corpus, n_embed=200, n_hidden=200, dropout=0.5, batch_size=128)\n",
    "learner.fit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我们现在在测试集上运行模型，可以看到准确率大概在34%左右\n",
    "* 之后我们可以查看针对测试集上的预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的结果 --> Loss: 3.374, Acc: 0.335, Words: 8059\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_words, test_result = learner.predict()\n",
    "print('测试集上的结果 --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "    test_loss, test_acc, test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测句子数量： 750\n",
      "------------------------------------------------------------\n",
      "结果样例：\n",
      "预测值\t [ 12  83 407  14  13   6 125   6   3 180  52   2   1]\n",
      "实际值\t [ 272    5  587   14    4  539   13   15    8  286 1126    2    1]\n",
      "预测句子\t He first end , he was how was to great time . </s>\n",
      "实际句子\t At the dentist , Bob learned he had a bad tooth . </s>\n"
     ]
    }
   ],
   "source": [
    "print('预测句子数量：', len(test_result['preds']))\n",
    "print('-' * 60)\n",
    "\n",
    "sample_index = 10\n",
    "print('结果样例：')\n",
    "print('预测值\\t', test_result['preds'][sample_index])\n",
    "print('实际值\\t', test_result['targets'][sample_index])\n",
    "print('预测句子\\t', corpus.index_to_sentence(test_result['preds'][sample_index]))\n",
    "print('实际句子\\t', corpus.index_to_sentence(test_result['targets'][sample_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 错误分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里我们统计了一下常见的35个预测错误（实际值，预测值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Bob', 'He'), 141),\n",
       " (('She', 'He'), 112),\n",
       " (('Sue', 'He'), 89),\n",
       " (('and', '.'), 60),\n",
       " (('to', '.'), 59),\n",
       " (('had', 'was'), 46),\n",
       " (('for', '.'), 42),\n",
       " (('decided', 'was'), 41),\n",
       " (('in', '.'), 37),\n",
       " (('his', 'the'), 31),\n",
       " ((',', '.'), 31),\n",
       " (('her', 'the'), 28),\n",
       " (('His', 'He'), 27),\n",
       " (('a', 'the'), 27),\n",
       " (('a', 'to'), 25),\n",
       " (('One', 'He'), 25),\n",
       " (('The', 'He'), 23),\n",
       " (('the', 'her'), 22),\n",
       " (('got', 'was'), 21),\n",
       " (('.', 'to'), 21),\n",
       " (('But', 'He'), 21),\n",
       " (('Her', 'He'), 21),\n",
       " (('went', 'was'), 20),\n",
       " ((\"'s\", 'was'), 19),\n",
       " (('When', 'He'), 19),\n",
       " (('!', '.'), 19),\n",
       " (('They', 'He'), 19),\n",
       " (('the', '.'), 19),\n",
       " (('at', '.'), 18),\n",
       " (('and', 'to'), 18),\n",
       " (('on', '.'), 17),\n",
       " (('wanted', 'was'), 17),\n",
       " (('the', 'a'), 16),\n",
       " (('he', 'to'), 16),\n",
       " (('with', '.'), 15)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistake_counter = Counter()\n",
    "for i in range(len(test_result['targets'])):\n",
    "    for j in range(len(test_result['targets'][i])):\n",
    "        pred, target = test_result['preds'][i][j], test_result['targets'][i][j]\n",
    "        if pred != target:\n",
    "            pred, target = corpus.vocab.itos[pred], corpus.vocab.itos[target]\n",
    "            mistake_counter[(target, pred)] += 1\n",
    "mistake_counter.most_common(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从上面的运行结果中，可以将主要错误类型分为以下几类：\n",
    "    1. **句首错误：**对于句子开头的单词模型会简单的预测为He，这主要是因为当句子开始时我们实际上只有简单的开始标记的信息，在没有其他上文信息的情况下，模型只能简单的预测训练集中常见的第一个词He\n",
    "    2. **介词错误：**对于句子中的介词(to,and,in etc)，模型倾向于预测为句号'.'，这部分应该是因为语言模型训练过程中没有办法看到文本后面的信息，模型在读入一部分单词之后很难判断后续是有介词连接的部分还是句子已经结束\n",
    "    3. **Be动词错误：**模型在一些情况下会将动词预测为'was'，这是由于训练语言模型时，单词从左向右输入，仅仅看到了主语很难判断主语之后的行为\n",
    "* * *\n",
    "* 总结一下，对于第一类错误可以通过添加上文信息来增加准确性，对于二、三类错误就不是简单语言模型能够解决的了，这两类错误需要后文信息来提高准确率，比如考虑使用BiLSTM之类的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('</s>', '</s>'), 745),\n",
       " (('.', '.'), 647),\n",
       " (('to', 'to'), 208),\n",
       " (('was', 'was'), 155),\n",
       " (('He', 'He'), 136),\n",
       " (('the', 'the'), 118),\n",
       " (('he', 'he'), 58),\n",
       " (('her', 'her'), 52),\n",
       " (('a', 'a'), 45),\n",
       " ((\"'t\", \"'t\"), 39),\n",
       " ((',', ','), 37),\n",
       " (('day', 'day'), 37),\n",
       " (('she', 'she'), 31),\n",
       " (('of', 'of'), 30),\n",
       " (('his', 'his'), 27),\n",
       " (('and', 'and'), 19),\n",
       " (('go', 'go'), 19),\n",
       " (('him', 'him'), 16),\n",
       " (('Bob', 'Bob'), 13),\n",
       " (('mom', 'mom'), 11),\n",
       " (('store', 'store'), 11),\n",
       " (('would', 'would'), 10),\n",
       " (('not', 'not'), 9),\n",
       " (('very', 'very'), 9),\n",
       " (('up', 'up'), 8),\n",
       " (('were', 'were'), 8),\n",
       " (('new', 'new'), 8),\n",
       " (('it', 'it'), 7),\n",
       " (('got', 'got'), 7),\n",
       " (('be', 'be'), 6),\n",
       " (('house', 'house'), 6),\n",
       " (('decided', 'decided'), 6),\n",
       " (('could', 'could'), 6),\n",
       " (('idea', 'idea'), 5),\n",
       " (('but', 'but'), 5)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_counter = Counter()\n",
    "for i in range(len(test_result['targets'])):\n",
    "    for j in range(len(test_result['targets'][i])):\n",
    "        pred, target = test_result['preds'][i][j], test_result['targets'][i][j]\n",
    "        if pred == target:\n",
    "            pred, target = corpus.vocab.itos[pred], corpus.vocab.itos[target]\n",
    "            correct_counter[(target, pred)] += 1\n",
    "correct_counter.most_common(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对于预测准确的部分，显然是句子什么时候结束预测的最好，毕竟出了句号'.'就能预测句子结尾标识了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "169px",
    "width": "230px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
