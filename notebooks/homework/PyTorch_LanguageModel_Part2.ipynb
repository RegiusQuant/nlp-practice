{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业一：语言模型 Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 书接上回，上一周实现了一个针对单句的语言模型，本周将在上周模型的基础上进一步探索\n",
    "* 本周的尝试主要分为两个部分\n",
    "    1. 采用BinaryLogLoss+负例采样\n",
    "    2. 使用额外的上下文信息进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先还是导入这次作业需要的包，并设置随机种子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(2020)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设定计算设备与数据集路径**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.4.0\n",
      "------------------------------------------------------------\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name:\n",
      "\t GeForce RTX 2080 Ti\n",
      "\t GeForce RTX 2080 Ti\n",
      "CUDA Current Device Index: 0\n",
      "------------------------------------------------------------\n",
      "Data Path: /media/bnu/data/nlp-practice/language-model\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "data_path = Path('/media/bnu/data/nlp-practice/language-model')\n",
    "\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "print('-' * 60)\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Device Count:', torch.cuda.device_count())\n",
    "    print('CUDA Device Name:')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print('\\t', torch.cuda.get_device_name(i))\n",
    "    print('CUDA Current Device Index:', torch.cuda.current_device())\n",
    "    print('-' * 60)\n",
    "print('Data Path:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryLogLoss+负例采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义单词表类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义`Vocab`类用于存储单词表\n",
    "* `Vocab`类当中包含了单词(token)与索引(index)之间的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab_path):\n",
    "        self.stoi = {}  # token -> index (dict)\n",
    "        self.itos = []  # index -> token (list)\n",
    "        \n",
    "        with open(vocab_path) as f:\n",
    "            # bobsue.voc.txt中，每一行是一个单词\n",
    "            for w in f.readlines():\n",
    "                w = w.strip()\n",
    "                if w not in self.stoi:\n",
    "                    self.stoi[w] = len(self.itos)\n",
    "                    self.itos.append(w)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词表大小： 1498\n",
      "------------------------------------------------------------\n",
      "样例（单词 -> 索引）：\n",
      "[('<s>', 0), ('</s>', 1), ('.', 2), ('to', 3), ('Bob', 4)]\n",
      "------------------------------------------------------------\n",
      "样例（索引 -> 单词）：\n",
      "[(0, '<s>'), (1, '</s>'), (2, '.'), (3, 'to'), (4, 'Bob')]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(data_path / 'bobsue.voc.txt')\n",
    "print('单词表大小：', len(vocab))\n",
    "print('-' * 60)\n",
    "print('样例（单词 -> 索引）：')\n",
    "print(list(vocab.stoi.items())[:5])\n",
    "print('-' * 60)\n",
    "print('样例（索引 -> 单词）：')\n",
    "print(list(enumerate(vocab.itos))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义`Corpus`类读取训练集、验证集、测试集语料\n",
    "* 语料文件中每一行都是一个句子，也就是我们训练时的一份样本\n",
    "* 将语料中的句子读入后，根据`Vocab`转换成索引列表\n",
    "* `Corpus`类当中包含了语料库中的单词的计数信息与词频信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, data_path, sort_by_len=False, \n",
    "                 uniform=False, freq_coef=0.75):\n",
    "        self.vocab = Vocab(data_path / 'bobsue.voc.txt')\n",
    "        self.sort_by_len = sort_by_len\n",
    "        self.train_data = self.tokenize(data_path / 'bobsue.lm.train.txt')\n",
    "        self.valid_data = self.tokenize(data_path / 'bobsue.lm.dev.txt')\n",
    "        self.test_data = self.tokenize(data_path / 'bobsue.lm.test.txt')\n",
    "\n",
    "        # 统计训练集的单词计数\n",
    "        self.word_counter = Counter()\n",
    "        for x in self.train_data:\n",
    "            # 注意<s>不在我们的预测范围内，不要统计\n",
    "            self.word_counter += Counter(x[1:])\n",
    "        # 训练集中需要预测的总词数\n",
    "        total_words = len(list(self.word_counter.elements()))\n",
    "\n",
    "        if uniform:  # 均匀分布\n",
    "            self.word_freqs = np.array(\n",
    "                [0.] + [1. for _ in range(len(self.vocab) - 1)],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            self.word_freqs = self.word_freqs / sum(self.word_freqs)\n",
    "        else:  # 词频分布（提升freq_coef次方）\n",
    "            self.word_freqs = np.array(\n",
    "                [self.word_counter[i] for i in range(len(self.vocab))],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            self.word_freqs = self.word_freqs / sum(self.word_freqs)\n",
    "            self.word_freqs = self.word_freqs ** freq_coef\n",
    "            self.word_freqs = self.word_freqs / sum(self.word_freqs)\n",
    "        \n",
    "    def tokenize(self, text_path):\n",
    "        with open(text_path) as f:\n",
    "            index_data = []  # 索引数据，存储每个样本的单词索引列表\n",
    "            for s in f.readlines():\n",
    "                index_data.append(\n",
    "                    self.sentence_to_index(s)\n",
    "                )\n",
    "        if self.sort_by_len:  # 为了提升训练速度，可以考虑将样本按照长度排序，这样可以减少padding\n",
    "            index_data = sorted(index_data, key=lambda x: len(x), reverse=True)\n",
    "        return index_data\n",
    "    \n",
    "    def sentence_to_index(self, s):\n",
    "        return [self.vocab.stoi[w] for w in s.split()]\n",
    "    \n",
    "    def index_to_sentence(self, x):\n",
    "        return ' '.join([self.vocab.itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 6036\n",
      "验证集句子数目： 750\n",
      "测试集句子数目： 750\n",
      "------------------------------------------------------------\n",
      "训练集总共单词数目： 71367\n",
      "验证集总共单词数目： 8707\n",
      "测试集总共单词数目： 8809\n",
      "------------------------------------------------------------\n",
      "训练集预测单词数目： 65331\n",
      "验证集预测单词数目： 7957\n",
      "测试集预测单词数目： 8059\n",
      "------------------------------------------------------------\n",
      "数据样本：\n",
      "[0, 16, 235, 372, 10, 60, 3, 75, 618, 39, 2, 1]\n",
      "<s> She ate quickly and asked to be taken home . </s>\n",
      "[0, 38, 192, 222, 32, 31, 4, 2, 1]\n",
      "<s> The girl broke up with Bob . </s>\n",
      "[0, 7, 842, 2, 1]\n",
      "<s> Sue apologized . </s>\n",
      "[0, 12, 150, 18, 8, 261, 3, 546, 102, 5, 1097, 2, 1]\n",
      "<s> He tried for a year to break into the market . </s>\n",
      "[0, 200, 706, 14, 5, 26, 15, 427, 228, 2, 1]\n",
      "<s> So far , the day had gone well . </s>\n",
      "------------------------------------------------------------\n",
      "\n",
      "均匀分布：\n",
      "词频样本： [0.       0.000668 0.000668 0.000668 0.000668]\n",
      "词频个数： 1498\n",
      "词频分布：\n",
      "词频样本： [0.         0.03825217 0.03720167 0.01996209 0.01417771]\n",
      "词频个数： 1498\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_path, sort_by_len=False)\n",
    "print('训练集句子数目：', len(corpus.train_data))\n",
    "print('验证集句子数目：', len(corpus.valid_data))\n",
    "print('测试集句子数目：', len(corpus.test_data))\n",
    "print('-' * 60)\n",
    "print('训练集总共单词数目：', sum([len(x) for x in corpus.train_data]))\n",
    "print('验证集总共单词数目：', sum([len(x) for x in corpus.valid_data]))\n",
    "print('测试集总共单词数目：', sum([len(x) for x in corpus.test_data]))\n",
    "print('-' * 60)\n",
    "print('训练集预测单词数目：', sum([len(x) - 1 for x in corpus.train_data]))\n",
    "print('验证集预测单词数目：', sum([len(x) - 1 for x in corpus.valid_data]))\n",
    "print('测试集预测单词数目：', sum([len(x) - 1 for x in corpus.test_data])) \n",
    "print('-' * 60)\n",
    "print('数据样本：')\n",
    "for i in range(5):\n",
    "    print(corpus.train_data[i])\n",
    "    print(corpus.index_to_sentence(corpus.train_data[i]))\n",
    "print('-' * 60)\n",
    "print()\n",
    "\n",
    "corpus = Corpus(data_path, sort_by_len=False, uniform=True)\n",
    "print('均匀分布：')\n",
    "print('词频样本：', corpus.word_freqs[:5])\n",
    "print('词频个数：', len(corpus.word_freqs))\n",
    "\n",
    "corpus = Corpus(data_path, sort_by_len=False, uniform=False, freq_coef=0.75)\n",
    "print('词频分布：')\n",
    "print('词频样本：', corpus.word_freqs[:5])\n",
    "print('词频个数：', len(corpus.word_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语言模型负例采样DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里使用PyTorch中的`DataSet`来构建我们自己的语言模型数据集\n",
    "* 我们自定义的类继承`DataSet`后，要实现`__len__`与`__getitem__`方法\n",
    "* 每个样本的输入是前n-1个单词，正例为后n-1个单词，负例根据词频和设定的生产个数进行生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BobSueNegSampleDataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, index_data, word_freqs, n_negs=20):\n",
    "        self.index_data = index_data  # 转换为序号的文本\n",
    "        self.n_negs = n_negs  # 生成负例个数\n",
    "        self.word_freqs = torch.FloatTensor(word_freqs)  # 词频\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        inputs = torch.LongTensor(self.index_data[i][:-1])\n",
    "        poss = torch.LongTensor(self.index_data[i][1:])\n",
    "        \n",
    "        # 生成n_negs个负例\n",
    "        negs = torch.zeros((len(poss), self.n_negs), dtype=torch.long)\n",
    "        for i in range(len(poss)):\n",
    "            negs[i] = torch.multinomial(self.word_freqs, self.n_negs)        \n",
    "        \n",
    "        return inputs, poss, negs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 6036\n",
      "\n",
      "训练集样本：\n",
      "输入大小： torch.Size([12])\n",
      "正例大小： torch.Size([12])\n",
      "负例大小： torch.Size([12, 20])\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_path, sort_by_len=False, uniform=False, freq_coef=0.75)\n",
    "train_set = BobSueNegSampleDataSet(corpus.train_data, corpus.word_freqs)\n",
    "print('训练集大小：', len(train_set))\n",
    "print()\n",
    "print('训练集样本：')\n",
    "print('输入大小：', train_set[10][0].shape)\n",
    "print('正例大小：', train_set[10][1].shape)\n",
    "print('负例大小：', train_set[10][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语言模型的DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这部分跟Part1相同，需要自定义collate_fn来处理这个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglm_collate_fn(batch):\n",
    "    # 首先将batch的格式进行转换\n",
    "    # batch[0]：Inputs\n",
    "    # batch[1]: Poss\n",
    "    # batch[2]: Negs\n",
    "    batch = list(zip(*batch))\n",
    "    \n",
    "    # lengths: (batch_size)\n",
    "    lengths = torch.LongTensor([len(x) for x in batch[0]])\n",
    "    # inputs: (batch_size, max_len)\n",
    "    inputs = nn.utils.rnn.pad_sequence(batch[0], batch_first=True)\n",
    "    # poss: (batch_size, max_len)\n",
    "    poss = nn.utils.rnn.pad_sequence(batch[1], batch_first=True)\n",
    "    # negs: (batch_size, max_len, n_negs)\n",
    "    negs = nn.utils.rnn.pad_sequence(batch[2], batch_first=True)\n",
    "    # mask: (batch_size, max_len)\n",
    "    mask = (poss != 0).float()\n",
    "    \n",
    "    return inputs, poss, negs, lengths, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([8, 16])\n",
      "Poss Shape: torch.Size([8, 16])\n",
      "Negs Shape: torch.Size([8, 16, 20])\n",
      "------------------------------------------------------------\n",
      "Lengths:\n",
      "tensor([12, 12,  9, 14,  9, 16,  9, 12])\n",
      "Mask:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=neglm_collate_fn\n",
    ")\n",
    "\n",
    "inputs, poss, negs, lengths, mask = next(iter(train_loader))\n",
    "print('Input Shape:', inputs.shape)\n",
    "print('Poss Shape:', poss.shape)\n",
    "print('Negs Shape:', negs.shape)\n",
    "print('-' * 60)\n",
    "print('Lengths:')\n",
    "print(lengths)\n",
    "print('Mask:')\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里实现一个基于LSTM的网络架构，Embedding层维度与LSTM隐含层维度相同\n",
    "* Inputs数据使用一个Embedding层，Postive和Negtive使用另一个Embedding层\n",
    "* 损失函数与Word2Vec中的负例采样损失相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegSampleLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, n_embed=200, dropout=0.5):\n",
    "        super(NegSampleLM, self).__init__()\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        # 输入的Embedding\n",
    "        self.embed_in = nn.Embedding(n_words, n_embed)\n",
    "        # 输出的Embedding\n",
    "        self.embed_out = nn.Embedding(n_words, n_embed)\n",
    "        # 这里embed_size一定要和hidden_size相同，为了之后点积计算loss\n",
    "        self.lstm = nn.LSTM(n_embed, n_embed, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, poss, negs, lengths, mask):\n",
    "        # x_embed: (batch_size, seq_len, embed_size)\n",
    "        x_embed = self.drop(self.embed_in(inputs))\n",
    "        # poss_embed: (batch_size, seq_len, embed_size)\n",
    "        poss_embed = self.embed_out(poss)\n",
    "        # negs_embed: (batch_size, seq_len, n_negs, embed_size)\n",
    "        negs_embed = self.embed_out(negs)\n",
    "        \n",
    "        x_embed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_embed, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # x_lstm: (batch_size, seq_len, embed_size)\n",
    "        x_lstm, _ = self.lstm(x_embed)        \n",
    "        x_lstm, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            x_lstm, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # x_lstm: (batch_size * seq_len, embed_size, 1)\n",
    "        x_lstm = x_lstm.view(-1, x_lstm.shape[2], 1)\n",
    "\n",
    "        # poss_embed: (batch_size * seq_len, 1, embed_size)\n",
    "        poss_embed = poss_embed.view(-1, 1, poss_embed.shape[2])\n",
    "        # negs_embed: (batch_size * seq_len, n_negs, embeds)\n",
    "        negs_embed = negs_embed.view(-1, negs_embed.shape[2], negs_embed.shape[3])\n",
    "        \n",
    "        # poss_mm: (batch_size * seq_len)\n",
    "        poss_mm = torch.bmm(poss_embed, x_lstm).squeeze()\n",
    "        # negs_mm: (batch_size * seq_len, n_negs)\n",
    "        negs_mm = torch.bmm(negs_embed, -x_lstm).squeeze()\n",
    "        \n",
    "        mask = mask.view(-1)\n",
    "        poss_loss = F.logsigmoid(poss_mm) * mask\n",
    "        negs_loss = F.logsigmoid(negs_mm).mean(1) * mask\n",
    "        \n",
    "        total_loss = -(poss_loss + negs_loss)\n",
    "        return total_loss.mean(), x_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失： 1.6722108125686646\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_path, sort_by_len=False, uniform=False, freq_coef=0.75)\n",
    "train_set = BobSueNegSampleDataSet(corpus.train_data, corpus.word_freqs)\n",
    "model = NegSampleLM(len(corpus.vocab), n_embed=200)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=neglm_collate_fn\n",
    ")\n",
    "inputs, poss, negs, lengths, mask = next(iter(train_loader))\n",
    "\n",
    "loss, x_lstm = model(inputs, poss, negs, lengths, mask)\n",
    "print('损失：', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义一个辅助函数用来生成预测\n",
    "* 这里采用LSTM后输出与输出部分的Embedding矩阵权重相乘，选取乘积最大的索引输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poss Shape: torch.Size([8, 14])\n",
      "Preds Shape: torch.Size([8, 14])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_prediction(model, x_lstm):\n",
    "    with torch.no_grad():\n",
    "        x_lstm = x_lstm.squeeze()  # (seq_len, embedding_size)\n",
    "        embed_weight = model.embed_out.weight.transpose(0, 1)  # (embedding_size, n_words)\n",
    "        preds = x_lstm @ embed_weight\n",
    "        preds = preds.argmax(dim=-1)\n",
    "    return preds\n",
    "\n",
    "preds = generate_prediction(model, x_lstm)\n",
    "preds = preds.view(-1, poss.shape[1])\n",
    "print('Poss Shape:', poss.shape)\n",
    "print('Preds Shape:', preds.shape)\n",
    "((preds == poss) * mask).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义基于负例采样的语言模型学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里为了之后评估模型训练时间方便，统一将BatchSize固定为1，这样在模型计算过程中也不用进行padding了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegSampleLearner:\n",
    "    \n",
    "    def __init__(self, corpus, n_embed=200, dropout=0.5, n_negs=20,\n",
    "                 batch_size=8):\n",
    "        self.corpus = corpus\n",
    "        self.model = NegSampleLM(len(corpus.vocab), n_embed, dropout).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.n_negs = n_negs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self, num_epochs):        \n",
    "        train_set = BobSueNegSampleDataSet(\n",
    "            self.corpus.train_data,\n",
    "            self.corpus.word_freqs,\n",
    "            n_negs=self.n_negs,\n",
    "        )\n",
    "        valid_set = BobSueNegSampleDataSet(\n",
    "            self.corpus.valid_data,\n",
    "            self.corpus.word_freqs,\n",
    "            n_negs=self.n_negs\n",
    "        )\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=neglm_collate_fn\n",
    "        )\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            dataset=valid_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=neglm_collate_fn\n",
    "        )\n",
    "        \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = datetime.datetime.now()\n",
    "            train_loss, train_words = self._make_train_step(train_loader)\n",
    "            end_time = datetime.datetime.now()\n",
    "            print(f'Epoch {epoch+1}:')\n",
    "            print('Train Step --> Loss: {:.3f}, Words: {}, Time: {}s'.format(\n",
    "                train_loss, train_words, (end_time-start_time).seconds\n",
    "            ))\n",
    "            \n",
    "            valid_loss, valid_acc, valid_words = self._make_valid_step(valid_loader)\n",
    "            print('Valid Step --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "                valid_loss, valid_acc, valid_words\n",
    "            ))\n",
    "            \n",
    "        \n",
    "    def _make_train_step(self, train_loader):\n",
    "        # 训练模式\n",
    "        self.model.train()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 预测单词总数\n",
    "        total_words = 0\n",
    "        \n",
    "        for inputs, poss, negs, lengths, mask in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            poss = poss.to(device)\n",
    "            negs = negs.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            # 模型损失\n",
    "            loss, _ = self.model(inputs, poss, negs, lengths, mask)\n",
    "            \n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # 统计信息\n",
    "            sent_words = lengths.sum().item()\n",
    "            total_words += sent_words\n",
    "            total_loss += loss.item() * sent_words\n",
    "        return total_loss / total_words, total_words\n",
    "    \n",
    "    def _make_valid_step(self, valid_loader):\n",
    "        # 验证模式\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = 0.0\n",
    "        # 预测正确个数，预测单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, poss, negs, lengths, mask in valid_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                poss = poss.to(device)\n",
    "                negs = negs.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                mask = mask.to(device)\n",
    "                \n",
    "                # 模型损失\n",
    "                loss, x_lstm = self.model(inputs, poss, negs, lengths, mask)\n",
    "                \n",
    "                # 生成预测，计算准确率\n",
    "                preds = generate_prediction(self.model, x_lstm)\n",
    "                preds = preds.view(-1, poss.shape[1])\n",
    "                \n",
    "                total_correct += ((preds == poss) * mask).sum().item()\n",
    "                \n",
    "                # 统计信息\n",
    "                sent_words = lengths.sum().item()\n",
    "                total_words += sent_words\n",
    "                total_loss += loss.item() * sent_words\n",
    "        return total_loss / total_words, total_correct / total_words, total_words\n",
    "    \n",
    "    def predict(self):\n",
    "        test_set = BobSueNegSampleDataSet(\n",
    "            self.corpus.test_data,\n",
    "            self.corpus.word_freqs,\n",
    "            n_negs=self.n_negs\n",
    "        )\n",
    "        \n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=neglm_collate_fn\n",
    "        )\n",
    "        \n",
    "        # 验证模式\n",
    "        self.model.eval()\n",
    "\n",
    "        # 预测正确个数，预测单词总数\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, poss, negs, lengths, mask in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                poss = poss.to(device)\n",
    "                negs = negs.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                mask = mask.to(device)\n",
    "                \n",
    "                # 模型损失\n",
    "                loss, x_lstm = self.model(inputs, poss, negs, lengths, mask)\n",
    "                \n",
    "                # 生成预测，计算准确率\n",
    "                preds = generate_prediction(self.model, x_lstm)\n",
    "                preds = preds.view(-1, poss.shape[1])\n",
    "                \n",
    "                total_correct += ((preds == poss) * mask).sum().item()\n",
    "                \n",
    "                # 统计信息\n",
    "                sent_words = lengths.sum().item()\n",
    "                total_words += sent_words\n",
    "        return total_correct / total_words, total_words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 准备就绪，设定好参数开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Step --> Loss: 0.776, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.512, Acc: 0.193, Words: 7957\n",
      "Epoch 2:\n",
      "Train Step --> Loss: 0.489, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.455, Acc: 0.221, Words: 7957\n",
      "Epoch 3:\n",
      "Train Step --> Loss: 0.419, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.422, Acc: 0.239, Words: 7957\n",
      "Epoch 4:\n",
      "Train Step --> Loss: 0.376, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.409, Acc: 0.242, Words: 7957\n",
      "Epoch 5:\n",
      "Train Step --> Loss: 0.340, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.399, Acc: 0.234, Words: 7957\n",
      "Epoch 6:\n",
      "Train Step --> Loss: 0.315, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.396, Acc: 0.242, Words: 7957\n",
      "Epoch 7:\n",
      "Train Step --> Loss: 0.292, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.399, Acc: 0.245, Words: 7957\n",
      "Epoch 8:\n",
      "Train Step --> Loss: 0.274, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.402, Acc: 0.254, Words: 7957\n",
      "Epoch 9:\n",
      "Train Step --> Loss: 0.259, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.410, Acc: 0.244, Words: 7957\n",
      "Epoch 10:\n",
      "Train Step --> Loss: 0.244, Words: 65331, Time: 4s\n",
      "Valid Step --> Loss: 0.418, Acc: 0.251, Words: 7957\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "corpus = Corpus(data_path, sort_by_len=False, uniform=True, freq_coef=0.1)\n",
    "learner = NegSampleLearner(corpus, n_embed=200, dropout=0.5, n_negs=20)\n",
    "learner.fit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里简单的看一下训练10个Epoch后的测试集结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集预测总词数： 8059\n",
      "测试集预测准确率： 0.25139595483310584\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_word = learner.predict()\n",
    "print('测试集预测总词数：', test_word)\n",
    "print('测试集预测准确率：', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用上下文信息Context的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这部分的内容基于Part1的实现，不过这次采用的上文信息\n",
    "* 我们需要重新构建我们的数据集，并将上文信息输入到模型中用于提高准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里我们在Part1的基础上将上下文句子按照制表符划分保存到对应的data列表里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextCorpus:\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.vocab = Vocab(data_path / 'bobsue.voc.txt')\n",
    "        self.train_data = self.tokenize(data_path / 'bobsue.prevsent.train.tsv')\n",
    "        self.valid_data = self.tokenize(data_path / 'bobsue.prevsent.dev.tsv')\n",
    "        self.test_data = self.tokenize(data_path / 'bobsue.prevsent.test.tsv')\n",
    "    \n",
    "    def tokenize(self, text_path):\n",
    "        with open(text_path) as f:\n",
    "            index_data = []\n",
    "            for s in f.readlines():\n",
    "                t = s.split('\\t')\n",
    "                index_data.append(\n",
    "                    (self.sentence_to_index(t[0]),\n",
    "                     self.sentence_to_index(t[1]))\n",
    "                )\n",
    "            return index_data\n",
    "                \n",
    "    def sentence_to_index(self, s):\n",
    "        return [self.vocab.stoi[w] for w in s.split()]\n",
    "    \n",
    "    def index_to_sentence(self, x):\n",
    "        return ' '.join([self.vocab.itos[i] for i in x])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 6036\n",
      "验证集句子数目： 750\n",
      "测试集句子数目： 750\n",
      "------------------------------------------------------------\n",
      "数据样本：\n",
      "[0, 7, 157, 17, 6, 103, 275, 2, 1]\n",
      "[0, 16, 235, 372, 10, 60, 3, 75, 618, 39, 2, 1]\n",
      "<s> Sue realized she was really bored . </s>\n",
      "<s> She ate quickly and asked to be taken home . </s>\n",
      "[0, 4, 211, 19, 505, 31, 8, 192, 2, 1]\n",
      "[0, 38, 192, 222, 32, 31, 4, 2, 1]\n",
      "<s> Bob fell in love with a girl . </s>\n",
      "<s> The girl broke up with Bob . </s>\n",
      "[0, 586, 9, 51, 65, 9, 17, 170, 23, 215, 227, 2, 1]\n",
      "[0, 7, 842, 2, 1]\n",
      "<s> Eventually her friend told her she wasn 't having fun . </s>\n",
      "<s> Sue apologized . </s>\n",
      "[0, 38, 518, 6, 123, 363, 226, 4, 2, 1]\n",
      "[0, 12, 150, 18, 8, 261, 3, 546, 102, 5, 1097, 2, 1]\n",
      "<s> The company was called pizza man Bob . </s>\n",
      "<s> He tried for a year to break into the market . </s>\n",
      "[0, 4, 6, 757, 25, 8, 37, 42, 2, 1]\n",
      "[0, 200, 706, 14, 5, 26, 15, 427, 228, 2, 1]\n",
      "<s> Bob was starting at a new school . </s>\n",
      "<s> So far , the day had gone well . </s>\n"
     ]
    }
   ],
   "source": [
    "corpus = ContextCorpus(data_path)\n",
    "print('训练集句子数目：', len(corpus.train_data))\n",
    "print('验证集句子数目：', len(corpus.valid_data))\n",
    "print('测试集句子数目：', len(corpus.test_data))\n",
    "print('-' * 60)\n",
    "\n",
    "print('数据样本：')\n",
    "for i in range(5):\n",
    "    print(corpus.train_data[i][0])\n",
    "    print(corpus.train_data[i][1])\n",
    "    print(corpus.index_to_sentence(corpus.train_data[i][0]))\n",
    "    print(corpus.index_to_sentence(corpus.train_data[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义上下文语言模型的DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `DateSet`中包含了每句话的上文信息、输入信息和预测目标信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BobSueContextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, index_data):\n",
    "        self.index_data = index_data\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        contexts = torch.LongTensor(self.index_data[i][0])\n",
    "        inputs = torch.LongTensor(self.index_data[i][1][:-1])\n",
    "        targets = torch.LongTensor(self.index_data[i][1][1:])\n",
    "        return contexts, inputs, targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 6036\n",
      "训练集样本：\n",
      "\t上文： [0, 16, 1303, 5, 709, 507, 3, 11, 1240, 107, 2, 1]\n",
      "\t <s> She followed the cute guy to his science class . </s>\n",
      "\t输入： [0, 7, 208, 601, 28, 10, 1276, 25, 5, 709, 507, 2]\n",
      "\t <s> Sue sat behind him and stared at the cute guy .\n",
      "\t输出： [7, 208, 601, 28, 10, 1276, 25, 5, 709, 507, 2, 1]\n",
      "\t Sue sat behind him and stared at the cute guy . </s>\n"
     ]
    }
   ],
   "source": [
    "train_set = BobSueContextDataset(corpus.train_data)\n",
    "print('训练集大小：', len(train_set))\n",
    "print('训练集样本：')\n",
    "contexts, inputs, targets = train_set[10]\n",
    "print('\\t上文：', list(contexts.numpy()))\n",
    "print('\\t', corpus.index_to_sentence(contexts.numpy()))\n",
    "print('\\t输入：', list(inputs.numpy()))\n",
    "print('\\t', corpus.index_to_sentence(inputs.numpy()))\n",
    "print('\\t输出：', list(targets.numpy()))\n",
    "print('\\t', corpus.index_to_sentence(targets.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义上下文语言模型的DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 与Part1相同，这里我们也通过自定义collate_fn来处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx_collate_fn(batch):\n",
    "    # 首先将batch的格式进行转换\n",
    "    # batch[0]：Contexts\n",
    "    # batch[1]: Inputs\n",
    "    # batch[2]: Targets\n",
    "    batch = list(zip(*batch))\n",
    "    \n",
    "    ctx_lengths = torch.LongTensor([len(x) for x in batch[0]])\n",
    "    inp_lengths = torch.LongTensor([len(x) for x in batch[1]])\n",
    "    \n",
    "    contexts = nn.utils.rnn.pad_sequence(batch[0], batch_first=True)\n",
    "    inputs = nn.utils.rnn.pad_sequence(batch[1], batch_first=True)\n",
    "    targets = nn.utils.rnn.pad_sequence(batch[2], batch_first=True)\n",
    "\n",
    "    mask = (targets != 0).float()\n",
    "    \n",
    "    return contexts, inputs, targets, ctx_lengths, inp_lengths, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexts Shape: torch.Size([8, 14])\n",
      "Inputs Shape: torch.Size([8, 16])\n",
      "Targets Shape: torch.Size([8, 16])\n",
      "------------------------------------------------------------\n",
      "Contexts Lengths：\n",
      "tensor([ 9, 10, 11,  9, 13, 14,  7, 13])\n",
      "Inputs Lengths: \n",
      "tensor([11, 12, 12, 10, 12, 16,  9, 10])\n",
      "------------------------------------------------------------\n",
      "Mask：\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=ctx_collate_fn\n",
    ")\n",
    "contexts, inputs, targets, ctx_lengths, inp_lengths, mask = next(iter(train_loader))\n",
    "print('Contexts Shape:', contexts.shape)\n",
    "print('Inputs Shape:', inputs.shape)\n",
    "print('Targets Shape:', targets.shape)\n",
    "print('-' * 60)\n",
    "print('Contexts Lengths：')\n",
    "print(ctx_lengths)\n",
    "print('Inputs Lengths: ')\n",
    "print(inp_lengths)\n",
    "print('-' * 60)\n",
    "print('Mask：')\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 和Part1类似定义一个LSTM的网络架构\n",
    "* 与Part1不同的是，这次我们先将context信息传入第一个LSTM，之后将最后一个hidden作为第二个LSTM的初始值，这样进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, n_embed=200, n_hidden=200, dropout=0.5):\n",
    "        super(ContextLM, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(n_words, n_embed)\n",
    "        self.encoder = nn.LSTM(n_embed, n_hidden, batch_first=True)\n",
    "        self.decoder = nn.LSTM(n_embed, n_hidden, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, n_words)\n",
    "        \n",
    "    def forward(self, contexts, inputs, ctx_lengths, inp_lengths):\n",
    "        # 对上一句话进行编码\n",
    "        ctx_emb = self.drop(self.embed(contexts))\n",
    "        ctx_emb = nn.utils.rnn.pack_padded_sequence(\n",
    "            ctx_emb, ctx_lengths, \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, c_n) = self.encoder(ctx_emb)\n",
    "        \n",
    "        # 对当前句子进行预测\n",
    "        inp_emb = self.drop(self.embed(inputs))\n",
    "        inp_emb = nn.utils.rnn.pack_padded_sequence(\n",
    "            inp_emb, inp_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        inp_out, _ = self.decoder(inp_emb, (h_n, c_n))\n",
    "        inp_out, _ = nn.utils.rnn.pad_packed_sequence(inp_out, batch_first=True)\n",
    "            \n",
    "        return self.linear(self.drop(inp_out))\n",
    "        \n",
    "model = ContextLM(len(corpus.vocab), 200, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输出Shape： torch.Size([8, 16, 1498])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(contexts, inputs, ctx_lengths, inp_lengths)\n",
    "print('模型输出Shape：', outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskCrossEntropyLoss, self).__init__()\n",
    "        self.celoss = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, outputs, targets, mask):\n",
    "        # outputs shape: (batch_size * max_len, vocab_size)\n",
    "        outputs = outputs.view(-1, outputs.size(2))\n",
    "        # targets shape: (batch_size * max_len)\n",
    "        targets = targets.view(-1)\n",
    "        # mask shape: (batch_size * max_len)\n",
    "        mask = mask.view(-1)\n",
    "        loss = self.celoss(outputs, targets) * mask\n",
    "        return torch.sum(loss) / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.322925090789795\n"
     ]
    }
   ],
   "source": [
    "criterion = MaskCrossEntropyLoss()\n",
    "loss = criterion(outputs, targets, mask)\n",
    "print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义基于上文信息的语言模型学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里为了方便起见和上一问相同，每一个批量就单独一句话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextLearner:\n",
    "    \n",
    "    def __init__(self, corpus, n_embed=200, n_hidden=200, dropout=0.5,\n",
    "                 batch_size=128, early_stopping_round=5):\n",
    "        self.corpus = corpus\n",
    "        self.model = ContextLM(len(corpus.vocab), n_embed, n_hidden, dropout)\n",
    "        self.model.to(device)\n",
    "        self.criterion = MaskCrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.history = defaultdict(list)\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def fit(self, num_epoch):\n",
    "        train_set = BobSueContextDataset(\n",
    "            self.corpus.train_data\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=ctx_collate_fn\n",
    "        )\n",
    "        \n",
    "        valid_set = BobSueContextDataset(\n",
    "            self.corpus.valid_data\n",
    "        )\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            dataset=valid_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=ctx_collate_fn\n",
    "        )\n",
    "        \n",
    "        no_improve_round = 0\n",
    "        \n",
    "        for epoch in range(num_epoch):\n",
    "            train_loss, train_acc, train_words = self._make_train_step(train_loader)\n",
    "            print(f'Epoch {epoch+1}:')\n",
    "            print('Train Step --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "                train_loss, train_acc, train_words))\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            \n",
    "            valid_loss, valid_acc, valid_words = self._make_valid_step(valid_loader)\n",
    "            print('Valid Step --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "                valid_loss, valid_acc, valid_words))\n",
    "            \n",
    "            self.history['valid_loss'].append(valid_loss)\n",
    "            self.history['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # 根据验证集的准确率进行EarlyStopping\n",
    "            if self.history['valid_acc'][-1] < max(self.history['valid_acc']):\n",
    "                no_improve_round += 1\n",
    "            else:\n",
    "                no_improve_round = 0\n",
    "            if no_improve_round == self.early_stopping_round:\n",
    "                print(f'Early Stopping at Epoch {epoch+1}')\n",
    "                break\n",
    "            \n",
    "            \n",
    "    def _make_train_step(self, train_loader):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            contexts = batch[0].to(device)\n",
    "            inputs = batch[1].to(device)\n",
    "            targets = batch[2].to(device)\n",
    "            ctx_lengths = batch[3].to(device)\n",
    "            inp_lengths = batch[4].to(device)\n",
    "            mask = batch[5].to(device)\n",
    "            \n",
    "            outputs = self.model(contexts, inputs, ctx_lengths, inp_lengths)\n",
    "            loss = self.criterion(outputs, targets, mask)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "            total_words += torch.sum(inp_lengths).item()\n",
    "            total_loss += loss.item() * torch.sum(mask).item()\n",
    "        \n",
    "        return total_loss / total_words, total_correct / total_words, total_words\n",
    "    \n",
    "    def _make_valid_step(self, valid_loader):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_correct, total_words = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                contexts = batch[0].to(device)\n",
    "                inputs = batch[1].to(device)\n",
    "                targets = batch[2].to(device)\n",
    "                ctx_lengths = batch[3].to(device)\n",
    "                inp_lengths = batch[4].to(device)\n",
    "                mask = batch[5].to(device)\n",
    "\n",
    "                outputs = self.model(contexts, inputs, ctx_lengths, inp_lengths)\n",
    "                loss = self.criterion(outputs, targets, mask)\n",
    "\n",
    "                total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "                total_words += torch.sum(inp_lengths).item()\n",
    "                total_loss += loss.item() * torch.sum(mask).item()\n",
    "        \n",
    "        return total_loss / total_words, total_correct / total_words, total_words\n",
    "    \n",
    "    def predict(self):\n",
    "        test_set = BobSueContextDataset(\n",
    "            self.corpus.test_data\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=test_set,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=ctx_collate_fn\n",
    "        )\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_correct, total_words = 0, 0\n",
    "        test_result = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                contexts = batch[0].to(device)\n",
    "                inputs = batch[1].to(device)\n",
    "                targets = batch[2].to(device)\n",
    "                ctx_lengths = batch[3].to(device)\n",
    "                inp_lengths = batch[4].to(device)\n",
    "                mask = batch[5].to(device)\n",
    "\n",
    "                outputs = self.model(contexts, inputs, ctx_lengths, inp_lengths)\n",
    "                loss = self.criterion(outputs, targets, mask)\n",
    "\n",
    "                total_correct += (outputs.argmax(-1) == targets).sum().item()\n",
    "                total_words += torch.sum(inp_lengths).item()\n",
    "                total_loss += loss.item() * torch.sum(mask).item()\n",
    "        \n",
    "                test_result['preds'].append(outputs.argmax(-1).data.cpu().numpy()[0])\n",
    "                test_result['targets'].append(targets.data.cpu().numpy()[0])\n",
    "            \n",
    "        return total_loss / total_words, total_correct / total_words, total_words, test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里设定好参数就可以开始训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Step --> Loss: 5.705, Acc: 0.161, Words: 65331\n",
      "Valid Step --> Loss: 4.661, Acc: 0.229, Words: 7957\n",
      "Epoch 2:\n",
      "Train Step --> Loss: 4.636, Acc: 0.227, Words: 65331\n",
      "Valid Step --> Loss: 4.372, Acc: 0.243, Words: 7957\n",
      "Epoch 3:\n",
      "Train Step --> Loss: 4.392, Acc: 0.243, Words: 65331\n",
      "Valid Step --> Loss: 4.157, Acc: 0.260, Words: 7957\n",
      "Epoch 4:\n",
      "Train Step --> Loss: 4.201, Acc: 0.256, Words: 65331\n",
      "Valid Step --> Loss: 3.990, Acc: 0.287, Words: 7957\n",
      "Epoch 5:\n",
      "Train Step --> Loss: 4.056, Acc: 0.272, Words: 65331\n",
      "Valid Step --> Loss: 3.871, Acc: 0.296, Words: 7957\n",
      "Epoch 6:\n",
      "Train Step --> Loss: 3.942, Acc: 0.283, Words: 65331\n",
      "Valid Step --> Loss: 3.776, Acc: 0.304, Words: 7957\n",
      "Epoch 7:\n",
      "Train Step --> Loss: 3.849, Acc: 0.296, Words: 65331\n",
      "Valid Step --> Loss: 3.698, Acc: 0.325, Words: 7957\n",
      "Epoch 8:\n",
      "Train Step --> Loss: 3.775, Acc: 0.307, Words: 65331\n",
      "Valid Step --> Loss: 3.636, Acc: 0.330, Words: 7957\n",
      "Epoch 9:\n",
      "Train Step --> Loss: 3.714, Acc: 0.313, Words: 65331\n",
      "Valid Step --> Loss: 3.590, Acc: 0.334, Words: 7957\n",
      "Epoch 10:\n",
      "Train Step --> Loss: 3.666, Acc: 0.318, Words: 65331\n",
      "Valid Step --> Loss: 3.551, Acc: 0.338, Words: 7957\n",
      "Epoch 11:\n",
      "Train Step --> Loss: 3.614, Acc: 0.323, Words: 65331\n",
      "Valid Step --> Loss: 3.525, Acc: 0.341, Words: 7957\n",
      "Epoch 12:\n",
      "Train Step --> Loss: 3.579, Acc: 0.326, Words: 65331\n",
      "Valid Step --> Loss: 3.492, Acc: 0.346, Words: 7957\n",
      "Epoch 13:\n",
      "Train Step --> Loss: 3.538, Acc: 0.330, Words: 65331\n",
      "Valid Step --> Loss: 3.471, Acc: 0.350, Words: 7957\n",
      "Epoch 14:\n",
      "Train Step --> Loss: 3.506, Acc: 0.331, Words: 65331\n",
      "Valid Step --> Loss: 3.448, Acc: 0.349, Words: 7957\n",
      "Epoch 15:\n",
      "Train Step --> Loss: 3.475, Acc: 0.335, Words: 65331\n",
      "Valid Step --> Loss: 3.434, Acc: 0.351, Words: 7957\n",
      "Epoch 16:\n",
      "Train Step --> Loss: 3.443, Acc: 0.337, Words: 65331\n",
      "Valid Step --> Loss: 3.421, Acc: 0.355, Words: 7957\n",
      "Epoch 17:\n",
      "Train Step --> Loss: 3.423, Acc: 0.337, Words: 65331\n",
      "Valid Step --> Loss: 3.407, Acc: 0.358, Words: 7957\n",
      "Epoch 18:\n",
      "Train Step --> Loss: 3.394, Acc: 0.340, Words: 65331\n",
      "Valid Step --> Loss: 3.385, Acc: 0.355, Words: 7957\n",
      "Epoch 19:\n",
      "Train Step --> Loss: 3.371, Acc: 0.343, Words: 65331\n",
      "Valid Step --> Loss: 3.378, Acc: 0.359, Words: 7957\n",
      "Epoch 20:\n",
      "Train Step --> Loss: 3.346, Acc: 0.346, Words: 65331\n",
      "Valid Step --> Loss: 3.372, Acc: 0.361, Words: 7957\n",
      "Epoch 21:\n",
      "Train Step --> Loss: 3.328, Acc: 0.346, Words: 65331\n",
      "Valid Step --> Loss: 3.355, Acc: 0.361, Words: 7957\n",
      "Epoch 22:\n",
      "Train Step --> Loss: 3.302, Acc: 0.351, Words: 65331\n",
      "Valid Step --> Loss: 3.351, Acc: 0.363, Words: 7957\n",
      "Epoch 23:\n",
      "Train Step --> Loss: 3.289, Acc: 0.349, Words: 65331\n",
      "Valid Step --> Loss: 3.338, Acc: 0.364, Words: 7957\n",
      "Epoch 24:\n",
      "Train Step --> Loss: 3.264, Acc: 0.353, Words: 65331\n",
      "Valid Step --> Loss: 3.332, Acc: 0.366, Words: 7957\n",
      "Epoch 25:\n",
      "Train Step --> Loss: 3.249, Acc: 0.353, Words: 65331\n",
      "Valid Step --> Loss: 3.327, Acc: 0.367, Words: 7957\n",
      "Epoch 26:\n",
      "Train Step --> Loss: 3.231, Acc: 0.355, Words: 65331\n",
      "Valid Step --> Loss: 3.324, Acc: 0.367, Words: 7957\n",
      "Epoch 27:\n",
      "Train Step --> Loss: 3.209, Acc: 0.358, Words: 65331\n",
      "Valid Step --> Loss: 3.315, Acc: 0.368, Words: 7957\n",
      "Epoch 28:\n",
      "Train Step --> Loss: 3.196, Acc: 0.358, Words: 65331\n",
      "Valid Step --> Loss: 3.313, Acc: 0.366, Words: 7957\n",
      "Epoch 29:\n",
      "Train Step --> Loss: 3.177, Acc: 0.359, Words: 65331\n",
      "Valid Step --> Loss: 3.301, Acc: 0.369, Words: 7957\n",
      "Epoch 30:\n",
      "Train Step --> Loss: 3.165, Acc: 0.361, Words: 65331\n",
      "Valid Step --> Loss: 3.307, Acc: 0.369, Words: 7957\n",
      "Epoch 31:\n",
      "Train Step --> Loss: 3.145, Acc: 0.362, Words: 65331\n",
      "Valid Step --> Loss: 3.299, Acc: 0.370, Words: 7957\n",
      "Epoch 32:\n",
      "Train Step --> Loss: 3.131, Acc: 0.364, Words: 65331\n",
      "Valid Step --> Loss: 3.294, Acc: 0.368, Words: 7957\n",
      "Epoch 33:\n",
      "Train Step --> Loss: 3.121, Acc: 0.365, Words: 65331\n",
      "Valid Step --> Loss: 3.291, Acc: 0.374, Words: 7957\n",
      "Epoch 34:\n",
      "Train Step --> Loss: 3.107, Acc: 0.365, Words: 65331\n",
      "Valid Step --> Loss: 3.289, Acc: 0.370, Words: 7957\n",
      "Epoch 35:\n",
      "Train Step --> Loss: 3.087, Acc: 0.368, Words: 65331\n",
      "Valid Step --> Loss: 3.288, Acc: 0.370, Words: 7957\n",
      "Epoch 36:\n",
      "Train Step --> Loss: 3.079, Acc: 0.370, Words: 65331\n",
      "Valid Step --> Loss: 3.280, Acc: 0.374, Words: 7957\n",
      "Epoch 37:\n",
      "Train Step --> Loss: 3.068, Acc: 0.368, Words: 65331\n",
      "Valid Step --> Loss: 3.283, Acc: 0.371, Words: 7957\n",
      "Epoch 38:\n",
      "Train Step --> Loss: 3.051, Acc: 0.369, Words: 65331\n",
      "Valid Step --> Loss: 3.285, Acc: 0.372, Words: 7957\n",
      "Epoch 39:\n",
      "Train Step --> Loss: 3.039, Acc: 0.373, Words: 65331\n",
      "Valid Step --> Loss: 3.284, Acc: 0.372, Words: 7957\n",
      "Epoch 40:\n",
      "Train Step --> Loss: 3.028, Acc: 0.373, Words: 65331\n",
      "Valid Step --> Loss: 3.286, Acc: 0.372, Words: 7957\n",
      "Epoch 41:\n",
      "Train Step --> Loss: 3.013, Acc: 0.373, Words: 65331\n",
      "Valid Step --> Loss: 3.276, Acc: 0.373, Words: 7957\n",
      "Early Stopping at Epoch 41\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "corpus = ContextCorpus(data_path)\n",
    "learner = ContextLearner(corpus, n_embed=200, n_hidden=200, dropout=0.5)\n",
    "learner.fit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 通过上面的训练，我们看一下测试集上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的结果 --> Loss: 3.329, Acc: 0.367, Words: 8059\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_words, test_result = learner.predict()\n",
    "print('测试集上的结果 --> Loss: {:.3f}, Acc: {:.3f}, Words: {}'.format(\n",
    "    test_loss, test_acc, test_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们在测试集上的表现已经超越了Part1当中的情况，在Part1中我们的准确率是0.341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测句子数量： 750\n",
      "------------------------------------------------------------\n",
      "结果样例：\n",
      "预测值\t [ 12   6   8   6  23 394  84   1]\n",
      "实际值\t [ 12 225  92 170  23 134   2   1]\n",
      "预测句子\t He was a was 't sure money </s>\n",
      "实际句子\t He noticed there wasn 't much . </s>\n"
     ]
    }
   ],
   "source": [
    "print('预测句子数量：', len(test_result['preds']))\n",
    "print('-' * 60)\n",
    "\n",
    "sample_index = 4\n",
    "print('结果样例：')\n",
    "print('预测值\\t', test_result['preds'][sample_index])\n",
    "print('实际值\\t', test_result['targets'][sample_index])\n",
    "print('预测句子\\t', corpus.index_to_sentence(test_result['preds'][sample_index]))\n",
    "print('实际句子\\t', corpus.index_to_sentence(test_result['targets'][sample_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 错误分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里我们统计了一下常见的35个预测错误（实际值，预测值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('to', '.'), 50),\n",
       " (('had', 'was'), 49),\n",
       " (('decided', 'was'), 42),\n",
       " (('and', '.'), 35),\n",
       " (('his', 'the'), 35),\n",
       " (('her', 'the'), 34),\n",
       " (('for', '.'), 33),\n",
       " (('.', 'to'), 32),\n",
       " (('Bob', 'He'), 31),\n",
       " (('in', '.'), 26),\n",
       " (('Sue', 'Bob'), 25),\n",
       " (('a', 'the'), 24),\n",
       " (('His', 'He'), 23),\n",
       " (('the', 'a'), 23),\n",
       " (('got', 'was'), 22),\n",
       " (('a', 'to'), 22),\n",
       " (('went', 'was'), 21),\n",
       " (('he', 'to'), 21),\n",
       " (('Sue', 'She'), 20),\n",
       " (('.', 'and'), 19),\n",
       " ((',', '.'), 19),\n",
       " (('Her', 'She'), 19),\n",
       " (('!', '.'), 18),\n",
       " (('at', '.'), 17),\n",
       " ((\"'s\", 'was'), 17),\n",
       " (('wanted', 'was'), 17),\n",
       " (('and', 'to'), 17),\n",
       " (('for', 'to'), 16),\n",
       " (('the', '.'), 16),\n",
       " (('asked', 'was'), 16),\n",
       " (('.', 'the'), 15),\n",
       " (('didn', 'was'), 15),\n",
       " (('it', 'her'), 14),\n",
       " (('a', '.'), 14),\n",
       " (('on', '.'), 14)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistake_counter = Counter()\n",
    "for i in range(len(test_result['targets'])):\n",
    "    for j in range(len(test_result['targets'][i])):\n",
    "        pred, target = test_result['preds'][i][j], test_result['targets'][i][j]\n",
    "        if pred != target:\n",
    "            pred, target = corpus.vocab.itos[pred], corpus.vocab.itos[target]\n",
    "            mistake_counter[(target, pred)] += 1\n",
    "mistake_counter.most_common(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里我们可以看出，在Part1当中主要的句首错误已经得到了有效的缓解，但是对于介词错误和Be动词错误模型还是不能很好的预测\n",
    "* 这主要是由于后两类错误在没有后文信息的情况下很难作出判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
