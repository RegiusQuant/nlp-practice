{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业二：文本分类 Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 本次作业使用神经网络进行文本情感分类\n",
    "* 使用Stanford-Sentiment-Treebank电影评论作为数据集\n",
    "\n",
    "文件名|说明\n",
    ":-:|:-:\n",
    "senti.train.tsv | 训练数据\n",
    "senti.dev.tsv | 验证数据\n",
    "senti.test.tsv | 测试数据\n",
    "\n",
    "* 文件的每一行是一个句子，和该句子的情感分类，中间由tab分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先导入这次作业需要的包，并设置随机种子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import tqdm\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(2020)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设定计算设备与数据集路径**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.4.0\n",
      "------------------------------------------------------------\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name:\n",
      "\t GeForce RTX 2080 Ti\n",
      "\t GeForce RTX 2080 Ti\n",
      "CUDA Current Device Index: 0\n",
      "------------------------------------------------------------\n",
      "Data Path: /media/bnu/data/nlp-practice/sentiment-analysis/standford-sentiment-treebank\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "data_path = Path('/media/bnu/data/nlp-practice/sentiment-analysis/standford-sentiment-treebank')\n",
    "\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "print('-' * 60)\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Device Count:', torch.cuda.device_count())\n",
    "    print('CUDA Device Name:')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print('\\t', torch.cuda.get_device_name(i))\n",
    "    print('CUDA Current Device Index:', torch.cuda.current_device())\n",
    "    print('-' * 60)\n",
    "print('Data Path:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据集的Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据集中每一列的数据类型，用于传换成Tensor\n",
    "text_field = torchtext.data.Field(sequential=True, batch_first=True, include_lengths=True)\n",
    "label_field = torchtext.data.LabelField(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "\n",
    "# 将tsv数据构建为数据集\n",
    "train_set, valid_set, test_set = torchtext.data.TabularDataset.splits(\n",
    "    path=data_path,\n",
    "    train='senti.train.tsv',\n",
    "    validation='senti.dev.tsv',\n",
    "    test='senti.test.tsv',\n",
    "    format='tsv',\n",
    "    fields=[('text', text_field), ('label', label_field)]\n",
    ")\n",
    "\n",
    "# 以训练集数据，构建单词表\n",
    "text_field.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Dataset Example:\n",
      "Text: ['The', 'mesmerizing', 'performances', 'of', 'the', 'leads', 'keep', 'the', 'film', 'grounded', 'and', 'keep', 'the', 'audience', 'riveted', '.']\n",
      "Label: 1\n",
      "------------------------------------------------------------\n",
      "Vocab: Str -> Index\n",
      "[('<unk>', 0), ('<pad>', 1), (',', 2), ('the', 3), ('and', 4)]\n",
      "Vocab: Index -> Str\n",
      "['<unk>', '<pad>', ',', 'the', 'and']\n",
      "Vocab Size:\n",
      "16284\n"
     ]
    }
   ],
   "source": [
    "print('Tabular Dataset Example:')\n",
    "print('Text:', valid_set[10].text)\n",
    "print('Label:', valid_set[10].label)\n",
    "print('-' * 60)\n",
    "\n",
    "print('Vocab: Str -> Index')\n",
    "print(list(text_field.vocab.stoi.items())[:5])\n",
    "print('Vocab: Index -> Str')\n",
    "print(text_field.vocab.itos[:5])\n",
    "print('Vocab Size:')\n",
    "print(len(text_field.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据集的Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    datasets=(train_set, valid_set, test_set),\n",
    "    batch_sizes=(256, 256, 256),\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iterator:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 256]\n",
      "\t[.text]:('[torch.cuda.LongTensor of size 256x9 (GPU 0)]', '[torch.cuda.LongTensor of size 256 (GPU 0)]')\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "------------------------------------------------------------ \n",
      "\n",
      "Valid Iterator:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 256]\n",
      "\t[.text]:('[torch.cuda.LongTensor of size 256x14 (GPU 0)]', '[torch.cuda.LongTensor of size 256 (GPU 0)]')\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "------------------------------------------------------------ \n",
      "\n",
      "Test Iterator:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 256]\n",
      "\t[.text]:('[torch.cuda.LongTensor of size 256x9 (GPU 0)]', '[torch.cuda.LongTensor of size 256 (GPU 0)]')\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train Iterator:')\n",
    "for batch in train_iter:\n",
    "    print(batch)\n",
    "    print('-' * 60, '\\n')\n",
    "    break\n",
    "    \n",
    "print('Valid Iterator:')\n",
    "for batch in valid_iter:\n",
    "    print(batch)\n",
    "    print('-' * 60, '\\n')\n",
    "    break\n",
    "    \n",
    "print('Test Iterator:')\n",
    "for batch in test_iter:\n",
    "    print(batch)\n",
    "    print('-' * 60, '\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量平均模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedAvgModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, n_embed, p_drop, pad_idx):\n",
    "        super(EmbedAvgModel, self).__init__()\n",
    "        self.embed = nn.Embedding(n_words, n_embed, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(n_embed, 1)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "    def forward(self, inputs, mask):\n",
    "        # (batch, len, n_embed)\n",
    "        inp_embed = self.drop(self.embed(inputs))\n",
    "        # (batch, len, 1)\n",
    "        mask = mask.float().unsqueeze(2)\n",
    "        # (batch, len, n_embed)\n",
    "        inp_embed = inp_embed * mask\n",
    "        # (batch, n_embed)\n",
    "        sum_embed = inp_embed.sum(1) / (mask.sum(1) + 1e-5)\n",
    "        return self.linear(sum_embed).squeeze()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedAvgModel(\n",
       "  (embed): Embedding(16284, 100, padding_idx=1)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmbedAvgModel(\n",
    "    n_words=len(text_field.vocab),\n",
    "    n_embed=100,\n",
    "    p_drop=0.2,\n",
    "    pad_idx=text_field.vocab.stoi['<pad>']\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention加权平均模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnAvgModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, n_embed, p_drop, pad_idx):\n",
    "        super(AttnAvgModel, self).__init__()\n",
    "        self.embed = nn.Embedding(n_words, n_embed, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(n_embed, 1)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.coef = nn.Parameter(torch.randn(1, 1, n_embed))\n",
    "\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        # (batch, len, n_embed)\n",
    "        inp_embed = self.embed(inputs)\n",
    "        # (batch, len)\n",
    "        inp_cos = F.cosine_similarity(inp_embed, self.coef, dim=-1)\n",
    "        inp_cos.masked_fill_(~mask, -1e5)\n",
    "        # (batch, 1, len)\n",
    "        inp_attn = F.softmax(inp_cos, dim=-1).unsqueeze(1)\n",
    "        # (batch, n_embed)\n",
    "        sum_embed = torch.bmm(inp_attn, inp_embed).squeeze()\n",
    "        sum_embed = self.drop(sum_embed)\n",
    "        return self.linear(sum_embed).squeeze()\n",
    "    \n",
    "    def calc_attention_weight(self, text):\n",
    "        # (1, len, n_embed)\n",
    "        inp_embed = self.embed(text)\n",
    "        # (1, len)\n",
    "        inp_cos = F.cosine_similarity(inp_embed, self.coef, dim=-1)\n",
    "        # (batch, 1, len)\n",
    "        inp_attn = F.softmax(inp_cos, dim=-1)\n",
    "        return inp_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "model = AttnAvgModel(\n",
    "    n_words=len(text_field.vocab),\n",
    "    n_embed=100,\n",
    "    p_drop=0.2,\n",
    "    pad_idx=text_field.vocab.stoi['<pad>']\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "for batch in train_iter:\n",
    "    inputs, lengths = batch.text\n",
    "    mask = (inputs != text_field.vocab.stoi['<pad>'])\n",
    "    outputs = model(inputs, mask)\n",
    "    print(outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCLearner:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3)\n",
    "        self.crirerion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def _calc_correct_num(self, outputs, targets):\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        return (preds == targets).int().sum().item()\n",
    "    \n",
    "    def fit(self, train_iter, valid_iter, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            total_sents, total_correct = 0, 0\n",
    "            \n",
    "            for batch in train_iter:\n",
    "                inputs, lengths = batch.text\n",
    "                targets = batch.label\n",
    "                mask = (inputs != text_field.vocab.stoi['<pad>'])\n",
    "                \n",
    "                outputs = self.model(inputs, mask)\n",
    "                loss = self.crirerion(outputs, targets)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * len(targets)\n",
    "                total_sents += len(targets)\n",
    "                total_correct += self._calc_correct_num(outputs, targets)\n",
    "            \n",
    "            epoch_loss = total_loss / total_sents\n",
    "            epoch_acc = total_correct / total_sents\n",
    "            print(f'Epoch {epoch+1}')\n",
    "            print(f'Train --> Loss: {epoch_loss:.3f}, Acc: {epoch_acc:.3f}')\n",
    "            \n",
    "            model.eval()\n",
    "            total_loss = 0.0\n",
    "            total_sents, total_correct = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_iter:\n",
    "                    inputs, lengths = batch.text\n",
    "                    targets = batch.label\n",
    "                    mask = (inputs != text_field.vocab.stoi['<pad>'])\n",
    "\n",
    "                    outputs = self.model(inputs, mask)\n",
    "                    loss = self.crirerion(outputs, targets)\n",
    "\n",
    "                    total_loss += loss.item() * len(targets)\n",
    "                    total_sents += len(targets)\n",
    "                    total_correct += self._calc_correct_num(outputs, targets)\n",
    "                \n",
    "            epoch_loss = total_loss / total_sents\n",
    "            epoch_acc = total_correct / total_sents\n",
    "            print(f'Valid --> Loss: {epoch_loss:.3f}, Acc: {epoch_acc:.3f}')\n",
    "        \n",
    "    def predict(self, test_iter):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_sents, total_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_iter:\n",
    "                inputs, lengths = batch.text\n",
    "                targets = batch.label\n",
    "                mask = (inputs != text_field.vocab.stoi['<pad>'])\n",
    "\n",
    "                outputs = self.model(inputs, mask)\n",
    "                loss = self.crirerion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * len(targets)\n",
    "                total_sents += len(targets)\n",
    "                total_correct += self._calc_correct_num(outputs, targets)\n",
    "\n",
    "        epoch_loss = total_loss / total_sents\n",
    "        epoch_acc = total_correct / total_sents\n",
    "        print(f'Test --> Loss: {epoch_loss:.3f}, Acc: {epoch_acc:.3f}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量平均模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train --> Loss: 0.662, Acc: 0.602\n",
      "Valid --> Loss: 0.635, Acc: 0.663\n",
      "Epoch 2\n",
      "Train --> Loss: 0.570, Acc: 0.720\n",
      "Valid --> Loss: 0.560, Acc: 0.750\n",
      "Epoch 3\n",
      "Train --> Loss: 0.479, Acc: 0.792\n",
      "Valid --> Loss: 0.508, Acc: 0.765\n",
      "Epoch 4\n",
      "Train --> Loss: 0.415, Acc: 0.828\n",
      "Valid --> Loss: 0.474, Acc: 0.784\n",
      "Epoch 5\n",
      "Train --> Loss: 0.369, Acc: 0.853\n",
      "Valid --> Loss: 0.454, Acc: 0.804\n",
      "Epoch 6\n",
      "Train --> Loss: 0.337, Acc: 0.870\n",
      "Valid --> Loss: 0.444, Acc: 0.803\n",
      "Epoch 7\n",
      "Train --> Loss: 0.315, Acc: 0.881\n",
      "Valid --> Loss: 0.434, Acc: 0.811\n",
      "Epoch 8\n",
      "Train --> Loss: 0.294, Acc: 0.887\n",
      "Valid --> Loss: 0.429, Acc: 0.820\n",
      "Epoch 9\n",
      "Train --> Loss: 0.274, Acc: 0.896\n",
      "Valid --> Loss: 0.428, Acc: 0.818\n",
      "Epoch 10\n",
      "Train --> Loss: 0.263, Acc: 0.901\n",
      "Valid --> Loss: 0.427, Acc: 0.820\n",
      "Test --> Loss: 0.424, Acc: 0.806\n"
     ]
    }
   ],
   "source": [
    "model = EmbedAvgModel(\n",
    "    n_words=len(text_field.vocab),\n",
    "    n_embed=200,\n",
    "    p_drop=0.5,\n",
    "    pad_idx=text_field.vocab.stoi['<pad>']\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "learner = TCLearner(model)\n",
    "learner.fit(train_iter, valid_iter, 10)\n",
    "learner.predict(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**单词 L2 Norm分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15个L2-Norm最小的单词：\n",
      "<pad>\n",
      "a\n",
      "finishing\n",
      "TV-insider\n",
      "field\n",
      "perpetrated\n",
      "clocks\n",
      "The\n",
      "Nelson\n",
      "cold-blooded\n",
      "shirt\n",
      "pic\n",
      "combat\n",
      "arctic\n",
      "abroad\n",
      "------------------------------------------------------------\n",
      "15个L2-Norm最大的单词：\n",
      "devoid\n",
      "sinks\n",
      "loses\n",
      "poorly\n",
      "meat\n",
      "gunfight\n",
      "wonderful\n",
      "lacks\n",
      "shallow\n",
      "Fails\n",
      "poor\n",
      "sweet\n",
      "ahead\n",
      "lacking\n",
      "worst\n"
     ]
    }
   ],
   "source": [
    "# (n_words)\n",
    "embed_norm = model.embed.weight.norm(dim=1)\n",
    "\n",
    "word_idx = list(range(len(text_field.vocab)))\n",
    "word_idx.sort(key=lambda x: embed_norm[x])\n",
    "\n",
    "print('15个L2-Norm最小的单词：')\n",
    "for i in word_idx[:15]:\n",
    "    print(text_field.vocab.itos[i])\n",
    "print('-' * 60)\n",
    "\n",
    "print('15个L2-Norm最大的单词：')\n",
    "for i in word_idx[-15:]:\n",
    "    print(text_field.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从上面的结果可以看出，L2-Norm小的单词往往是跟情感无关的单词\n",
    "* L2-Norm大的单词基本都是能够反映情感的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention加权平均模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train --> Loss: 0.668, Acc: 0.591\n",
      "Valid --> Loss: 0.644, Acc: 0.634\n",
      "Epoch 2\n",
      "Train --> Loss: 0.579, Acc: 0.712\n",
      "Valid --> Loss: 0.568, Acc: 0.745\n",
      "Epoch 3\n",
      "Train --> Loss: 0.480, Acc: 0.788\n",
      "Valid --> Loss: 0.509, Acc: 0.768\n",
      "Epoch 4\n",
      "Train --> Loss: 0.411, Acc: 0.831\n",
      "Valid --> Loss: 0.477, Acc: 0.780\n",
      "Epoch 5\n",
      "Train --> Loss: 0.363, Acc: 0.855\n",
      "Valid --> Loss: 0.459, Acc: 0.776\n",
      "Epoch 6\n",
      "Train --> Loss: 0.329, Acc: 0.871\n",
      "Valid --> Loss: 0.446, Acc: 0.788\n",
      "Epoch 7\n",
      "Train --> Loss: 0.303, Acc: 0.883\n",
      "Valid --> Loss: 0.442, Acc: 0.790\n",
      "Epoch 8\n",
      "Train --> Loss: 0.283, Acc: 0.893\n",
      "Valid --> Loss: 0.437, Acc: 0.798\n",
      "Epoch 9\n",
      "Train --> Loss: 0.264, Acc: 0.901\n",
      "Valid --> Loss: 0.437, Acc: 0.804\n",
      "Epoch 10\n",
      "Train --> Loss: 0.253, Acc: 0.904\n",
      "Valid --> Loss: 0.435, Acc: 0.802\n",
      "Test --> Loss: 0.425, Acc: 0.803\n"
     ]
    }
   ],
   "source": [
    "model = AttnAvgModel(\n",
    "    n_words=len(text_field.vocab),\n",
    "    n_embed=200,\n",
    "    p_drop=0.5,\n",
    "    pad_idx=text_field.vocab.stoi['<pad>']\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "learner = TCLearner(model)\n",
    "learner.fit(train_iter, valid_iter, 10)\n",
    "learner.predict(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分析计算向量u与词向量的余弦相似度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15个余弦相似度最小的单词：\n",
      "and\n",
      "the\n",
      ",\n",
      "of\n",
      "a\n",
      "banter-filled\n",
      "to\n",
      "taking\n",
      "zap\n",
      "Swim\n",
      "Sandra\n",
      "hiatus\n",
      "coming-of-age\n",
      "incredibly\n",
      "produces\n",
      "------------------------------------------------------------\n",
      "15个余弦相似度最大的单词：\n",
      "uninspired\n",
      "flat\n",
      "tedious\n",
      "violence\n",
      "scooped\n",
      "all-night\n",
      "Hollywood-itis\n",
      "poor\n",
      "devoid\n",
      "lacking\n",
      "fallen\n",
      "poorly\n",
      "neither\n",
      "lacks\n",
      "worst\n"
     ]
    }
   ],
   "source": [
    "# (1, n_embed)\n",
    "u = model.coef.view(1, -1)\n",
    "# (n_words, n_embed)\n",
    "embedding = model.embed.weight\n",
    "# (n_words)\n",
    "cos_sim = F.cosine_similarity(u, embedding, dim=-1)\n",
    "\n",
    "word_idx = list(range(len(text_field.vocab)))\n",
    "word_idx.sort(key=lambda x: cos_sim[x])\n",
    "\n",
    "print('15个余弦相似度最小的单词：')\n",
    "for i in word_idx[:15]:\n",
    "    print(text_field.vocab.itos[i])\n",
    "print('-' * 60)\n",
    "\n",
    "print('15个余弦相似度最大的单词：')\n",
    "for i in word_idx[-15:]:\n",
    "    print(text_field.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当面的结果可以看出，余弦相似度比较高的单词都能够很好的反映句子的情感，这些单词在Attention后的权重会比较高\n",
    "* 余弦相似度小的单词多为一些名词和介词，与文本表示的情感基本无关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分析训练数据中单词的Attention权重**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences finish!\n",
      "20000 sentences finish!\n",
      "30000 sentences finish!\n",
      "40000 sentences finish!\n",
      "50000 sentences finish!\n",
      "60000 sentences finish!\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    datasets=(train_set, valid_set, test_set),\n",
    "    batch_sizes=(1, 1, 1),\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "weight_dict = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k, batch in enumerate(train_iter):\n",
    "        inputs, lengths = batch.text\n",
    "        attn = model.calc_attention_weight(inputs)\n",
    "        inputs = inputs.view(-1)\n",
    "        attn = attn.view(-1)\n",
    "        if inputs.shape[0] == 1:\n",
    "            weight_dict[inputs.item()].append(attn.item())\n",
    "        else:\n",
    "            for i in range(len(inputs)):\n",
    "                weight_dict[inputs[i].item()].append(attn[i].item())\n",
    "        if (k + 1) % 10000 == 0:\n",
    "            print(f'{k+1} sentences finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dict, std_dict = {}, {}\n",
    "for k, v in weight_dict.items():\n",
    "    # 至少出现100次\n",
    "    if len(v) >= 100:\n",
    "        mean_dict[k] = np.mean(v)\n",
    "        std_dict[k] = np.std(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30个Attention标准差最大的单词：\n",
      "------------------------------------------------------------\n",
      "tedious, Freq:102, Std:0.187, Mean:0.204\n",
      "stupid, Freq:132, Std:0.180, Mean:0.211\n",
      "painful, Freq:108, Std:0.178, Mean:0.206\n",
      "mess, Freq:149, Std:0.174, Mean:0.227\n",
      "waste, Freq:114, Std:0.172, Mean:0.206\n",
      "pretentious, Freq:118, Std:0.172, Mean:0.205\n",
      "worse, Freq:122, Std:0.171, Mean:0.164\n",
      "flat, Freq:181, Std:0.171, Mean:0.192\n",
      "bland, Freq:140, Std:0.170, Mean:0.184\n",
      "appealing, Freq:124, Std:0.169, Mean:0.180\n",
      "provocative, Freq:103, Std:0.168, Mean:0.195\n",
      "unfunny, Freq:105, Std:0.167, Mean:0.212\n",
      "tired, Freq:135, Std:0.165, Mean:0.216\n",
      "convincing, Freq:110, Std:0.165, Mean:0.152\n",
      "hackneyed, Freq:101, Std:0.165, Mean:0.162\n",
      "gorgeous, Freq:133, Std:0.164, Mean:0.173\n",
      "dumb, Freq:167, Std:0.163, Mean:0.190\n",
      "epic, Freq:103, Std:0.163, Mean:0.162\n",
      "success, Freq:107, Std:0.163, Mean:0.163\n",
      "impressive, Freq:125, Std:0.163, Mean:0.184\n",
      "boring, Freq:169, Std:0.162, Mean:0.180\n",
      "creepy, Freq:114, Std:0.162, Mean:0.179\n",
      "stylish, Freq:108, Std:0.161, Mean:0.178\n",
      "cold, Freq:130, Std:0.161, Mean:0.173\n",
      "fine, Freq:186, Std:0.161, Mean:0.179\n",
      "terrific, Freq:147, Std:0.161, Mean:0.179\n",
      "endearing, Freq:101, Std:0.160, Mean:0.172\n",
      "visually, Freq:143, Std:0.160, Mean:0.168\n",
      "slow, Freq:143, Std:0.160, Mean:0.184\n",
      "sentimental, Freq:102, Std:0.160, Mean:0.159\n",
      "\n",
      "30个Attention标准差最小的单词：\n",
      "------------------------------------------------------------\n",
      ":, Freq:445, Std:0.049, Mean:0.070\n",
      "if, Freq:875, Std:0.049, Mean:0.066\n",
      "had, Freq:350, Std:0.049, Mean:0.080\n",
      "come, Freq:296, Std:0.049, Mean:0.069\n",
      "they, Freq:630, Std:0.049, Mean:0.065\n",
      "wo, Freq:165, Std:0.048, Mean:0.084\n",
      "knows, Freq:105, Std:0.048, Mean:0.079\n",
      "to, Freq:12483, Std:0.047, Mean:0.065\n",
      "know, Freq:202, Std:0.047, Mean:0.072\n",
      "now, Freq:115, Std:0.047, Mean:0.067\n",
      "having, Freq:144, Std:0.047, Mean:0.078\n",
      "would, Freq:605, Std:0.046, Mean:0.078\n",
      "why, Freq:260, Std:0.046, Mean:0.079\n",
      "because, Freq:429, Std:0.045, Mean:0.072\n",
      "might, Freq:373, Std:0.045, Mean:0.067\n",
      "Mr., Freq:143, Std:0.045, Mean:0.076\n",
      "we, Freq:536, Std:0.045, Mean:0.071\n",
      "'d, Freq:175, Std:0.045, Mean:0.085\n",
      ";, Freq:290, Std:0.044, Mean:0.068\n",
      "who, Freq:1260, Std:0.044, Mean:0.070\n",
      "'ve, Freq:330, Std:0.044, Mean:0.070\n",
      "where, Freq:248, Std:0.044, Mean:0.073\n",
      "Michael, Freq:115, Std:0.043, Mean:0.066\n",
      "that, Freq:7689, Std:0.042, Mean:0.062\n",
      "getting, Freq:100, Std:0.042, Mean:0.079\n",
      "If, Freq:175, Std:0.040, Mean:0.067\n",
      "while, Freq:356, Std:0.040, Mean:0.065\n",
      "There, Freq:220, Std:0.038, Mean:0.062\n",
      "when, Freq:452, Std:0.037, Mean:0.063\n",
      "which, Freq:842, Std:0.034, Mean:0.064\n"
     ]
    }
   ],
   "source": [
    "word_idx = list(std_dict.keys())\n",
    "word_idx.sort(key=lambda x: std_dict[x], reverse=True)\n",
    "print('30个Attention标准差最大的单词：')\n",
    "print('-' * 60)\n",
    "for i in word_idx[:30]:\n",
    "    print(f'{text_field.vocab.itos[i]}, Freq:{len(weight_dict[i])}, Std:{std_dict[i]:.3f}, Mean:{mean_dict[i]:.3f}')\n",
    "print()\n",
    "print('30个Attention标准差最小的单词：')\n",
    "print('-' * 60)\n",
    "for i in word_idx[-30:]:\n",
    "    print(f'{text_field.vocab.itos[i]}, Freq:{len(weight_dict[i])}, Std:{std_dict[i]:.3f}, Mean:{mean_dict[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Attention权重标准差大的单词，其权重的的平均值也很大\n",
    "* 这些标准差大的单词，往往会有明显的情感倾向，所以会有较大的权重平均值\n",
    "* 造成权重标准差大的原因主要是因为句子的长短，由于句子长短不同包含的情感倾向单词数目不同，造成权重变化很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
