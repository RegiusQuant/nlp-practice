{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(2020)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读取与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01623116, 0.01051   , 0.00803599, 0.00798072, 0.007389  ,\n",
       "       0.00666238, 0.00653963, 0.0056647 , 0.00547045, 0.00447826],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, corpus_path, vocab_size):\n",
    "        with open(corpus_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        self.text = [w for w in text.lower().split()]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = dict(Counter(self.text).most_common(vocab_size - 1))\n",
    "        self.vocab['<unk>'] = len(self.text) - sum(self.vocab.values())\n",
    "        self.itos = [w for w in self.vocab.keys()]\n",
    "        self.stoi = {w: i for i, w in enumerate(self.itos)}\n",
    "        \n",
    "    def get_word_freqs(self, freq_coef=0.75):\n",
    "        word_counts = np.array([c for c in self.vocab.values()], dtype=np.float32)\n",
    "        word_freqs = word_counts / np.sum(word_counts)\n",
    "        word_freqs = word_freqs ** freq_coef\n",
    "        word_freqs = word_freqs / np.sum(word_freqs)\n",
    "        return word_freqs\n",
    "    \n",
    "corpus_path = Path('/media/bnu/data/nlp-practice/language-model/text8.train.txt')\n",
    "corpus = Corpus(corpus_path, 30000)\n",
    "word_freqs = corpus.get_word_freqs()\n",
    "word_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 120])\n"
     ]
    }
   ],
   "source": [
    "class Word2VecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, win_size=3, neg_coef=20):\n",
    "        self.corpus = corpus\n",
    "        self.win_size = win_size\n",
    "        self.neg_coef = neg_coef\n",
    "        \n",
    "        unk_idx = corpus.stoi['<unk>']\n",
    "        self.text = [corpus.stoi.get(w, unk_idx) for w in corpus.text]\n",
    "        self.text = torch.LongTensor(self.text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text[self.win_size:-self.win_size])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        center_idx = i + self.win_size\n",
    "        center_word = self.text[center_idx]\n",
    "        \n",
    "        pos_idxs = (list(range(center_idx - self.win_size, center_idx)) +\n",
    "                    list(range(center_idx + 1, center_idx + self.win_size + 1)))\n",
    "        pos_words = self.text[pos_idxs]\n",
    "        \n",
    "        word_freqs = corpus.get_word_freqs()\n",
    "        word_freqs = torch.FloatTensor(word_freqs)\n",
    "        neg_words = torch.multinomial(word_freqs, self.neg_coef * len(pos_idxs), replacement=True)\n",
    "        \n",
    "        return center_word, pos_words, neg_words\n",
    "    \n",
    "dataset = Word2VecDataset(corpus)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "center_word, pos_words, neg_words = next(iter(dataloader))\n",
    "print(center_word.shape)\n",
    "print(pos_words.shape)\n",
    "print(neg_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, n_embed):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        self.inp_embed = nn.Embedding(n_words, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_words, n_embed)\n",
    "        \n",
    "    def forward(self, center_word, pos_words, neg_words):\n",
    "        # (batch_size, n_embed, 1)\n",
    "        ctr_embed = self.inp_embed(center_word).unsqueeze(2)\n",
    "        # (batch_size, n_pos, n_embed)\n",
    "        pos_embed = self.out_embed(pos_words)\n",
    "        # (batch.size, n_neg, n_embed)\n",
    "        neg_embed = self.out_embed(neg_words)\n",
    "        \n",
    "        # (batch_size, n_pos)\n",
    "        pos_bmm = torch.bmm(pos_embed, ctr_embed).squeeze()\n",
    "        # (batch_size, n_neg)\n",
    "        neg_bmm = torch.bmm(neg_embed, -ctr_embed).squeeze()\n",
    "        \n",
    "        pos_loss = -F.logsigmoid(pos_bmm).sum(1)\n",
    "        neg_loss = -F.logsigmoid(neg_bmm).sum(1)\n",
    "        return (pos_loss + neg_loss).mean()\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.inp_embed.weight.data.cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.022000427249910234"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(filename, embeddings):\n",
    "    if str(filename).endswith('.csv'):\n",
    "        data = pd.read_csv(filename, sep=',')\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "    human_sims, model_sims = [], []\n",
    "    for i in data.index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 in corpus.stoi and word2 in corpus.stoi:\n",
    "            word1_idx, word2_idx = corpus.stoi[word1], corpus.stoi[word2]\n",
    "            embed1, embed2 = embeddings[[word1_idx]], embeddings[[word2_idx]]\n",
    "            \n",
    "            model_sims.append(cosine_similarity(embed1, embed2)[0][0])\n",
    "            human_sims.append(data.iloc[i, 2])\n",
    "    return spearmanr(human_sims, model_sims).correlation\n",
    "    \n",
    "    \n",
    "eval_path = Path('/media/bnu/data/nlp-practice/word-vector')\n",
    "model = Word2VecModel(dataset.corpus.vocab_size, 300)\n",
    "embeddings = model.get_embeddings()\n",
    "\n",
    "evaluate(eval_path / 'simlex-999.txt', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = Word2VecModel(dataset.corpus.vocab_size, 100)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    pbar = tqdm.notebook.tqdm(dataloader)\n",
    "    pbar.set_description(f'Epoch {epoch+1} --> Train')\n",
    "    \n",
    "    corr = 0.0\n",
    "    for i, (center_word, pos_words, neg_words) in enumerate(pbar):\n",
    "        center_word = center_word.to(device)\n",
    "        pos_words = pos_words.to(device)\n",
    "        neg_words = neg_words.to(device)\n",
    "        \n",
    "        loss = model(center_word, pos_words, neg_words)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            embeddings = model.get_embeddings()\n",
    "            corr = evaluate(eval_path / 'simlex-999.txt', embeddings)\n",
    "            \n",
    "        pbar.set_postfix(loss=loss.item(), corr=corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
